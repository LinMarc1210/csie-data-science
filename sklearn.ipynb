{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "# 收集所有資料夾名稱，按照數字順序排序資料夾名稱\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "dataset_names = sorted(dataset_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for folder_name in dataset_names:\n",
    "    # print(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(len(dataset_names))\n",
    "# print(len(X_trains))  # 49, 代表有 49 個 dataFrame (每個資料集各一個)\n",
    "# print(len(y_trains))\n",
    "# print(len(X_tests))\n",
    "# print(X_trains[0].dtypes)\n",
    "# print(y_trains[0].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_trains[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# 對每組資料進行處理\n",
    "for i in range(len(dataset_names)):\n",
    "    # 將連續型資料和數值型資料標準化\n",
    "    numerical_df = X_trains[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "    categorical_df = X_trains[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "    if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "        print('Splitting error')\n",
    "    # numerical_df --> normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(numerical_df)\n",
    "    numerical_s = scaler.transform(numerical_df)\n",
    "    numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "    X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    \n",
    "    numerical_df = X_tests[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "    categorical_df = X_tests[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "    # 直接照前面用過的 scaler 來分\n",
    "    numerical_s = scaler.transform(numerical_df)\n",
    "    numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "    X_tests[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# def preprocess_data(df, scaler=None, label_binarizers=None, columns=None):\n",
    "#     # 將連續型資料和數值型資料標準化\n",
    "#     numerical_df = df.select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = df.select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "\n",
    "#     # numerical_df --> normalization\n",
    "#     if scaler is None:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "\n",
    "#     # categorial_df --> label binarizer encoding\n",
    "#     if label_binarizers is None:\n",
    "#         label_binarizers = {}\n",
    "\n",
    "#     encoded_cols = []\n",
    "#     for col in categorical_df.columns:\n",
    "#         unique_values = categorical_df[col].nunique()\n",
    "\n",
    "#         if col not in label_binarizers:\n",
    "#             c_scaler = LabelBinarizer()\n",
    "#             c_scaler.fit(categorical_df[col])\n",
    "#             label_binarizers[col] = c_scaler\n",
    "#         encoded_df = label_binarizers[col].transform(categorical_df[col])  # 轉成 ndarray\n",
    "\n",
    "#         # 如果是多類別，轉換成 DataFrame，並加上欄位名稱\n",
    "#         if encoded_df.shape[1] > 1:\n",
    "#             encoded_df = pd.DataFrame(encoded_df, columns=[f\"{col}_{cls}\" for cls in label_binarizers[col].classes_])\n",
    "#         else:\n",
    "#             encoded_df = pd.Series(encoded_df.flatten(), name=categorical_df[col].name)\n",
    "#         encoded_cols.append(encoded_df)\n",
    "\n",
    "#     encoded_df = pd.concat([categorical_df.drop(columns=categorical_df.columns)] + encoded_cols, axis=1)\n",
    "\n",
    "#     # 如果是測試資料，補齊缺少的欄位並重新排序\n",
    "#     if columns is not None:\n",
    "#         missing_cols = set(columns) - set(encoded_df.columns)\n",
    "#         for col in missing_cols:\n",
    "#             encoded_df[col] = 0  # 缺失的欄位補 0\n",
    "#         encoded_df = encoded_df[columns]  # 重新排序以匹配訓練資料的欄位順序\n",
    "\n",
    "#     # 合併數值型和類別型資料框\n",
    "#     processed_df = pd.concat([numerical_df, encoded_df], axis=1)\n",
    "#     processed_df = processed_df[columns] if columns is not None else processed_df\n",
    "\n",
    "#     return processed_df, scaler, label_binarizers\n",
    "\n",
    "# # 對每組資料進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     X_trains[i], n_scaler, label_binarizers = preprocess_data(X_trains[i])\n",
    "#     X_tests[i], _, _ = preprocess_data(X_tests[i], scaler=n_scaler, label_binarizers=label_binarizers, columns=X_trains[i].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset_names)):\n",
    "#     missing_cols = set(X_trains[i].columns) - set(X_tests[i].columns)\n",
    "#     print(missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8891371436622877\n"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "for i in range(len(dataset_names)):\n",
    "    # 這邊做一下 stratify\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "    # Random Forest (適合這次的小資料集)\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=400, n_jobs=1, random_state=1, max_depth=7,  \n",
    "    )\n",
    "\n",
    "    # # Neural Net\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128,64),\n",
    "    #     activation=\"relu\",\n",
    "    #     random_state=1,\n",
    "    #     alpha=0.01\n",
    "    # )\n",
    "    \n",
    "    # # XGBoost (好像比較容易過擬合，適合大資料集) \n",
    "    # model = XGBClassifier(\n",
    "    #     n_estimators=200, learning_rate=0.1, gamma=0.8,\n",
    "    # )\n",
    "    \n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "    auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    # print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "    \n",
    "print(avg_auc / len(dataset_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    # print(X_tests[i])\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# test_aucs = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用模型進行預測，獲得類別 1 的預測機率\n",
    "#     y_predict_proba = models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    \n",
    "#     # 計算 AUC 分數\n",
    "#     auc = roc_auc_score(y_tests[i], y_predict_proba)\n",
    "#     print(f'AUC of dataset {i:2}: \\t{auc}')\n",
    "    \n",
    "#     test_aucs.append(auc)\n",
    "\n",
    "# # 平均 AUC\n",
    "# avg_test_auc = sum(test_aucs) / len(test_aucs)\n",
    "# print(\"\\nAverage Test AUC:\", avg_test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
