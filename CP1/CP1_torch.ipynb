{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "# 收集所有資料夾名稱，按照數字順序排序資料夾名稱\n",
    "for folder_name in os.listdir(\"../Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "dataset_names = sorted(dataset_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for folder_name in dataset_names:\n",
    "    # print(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"../Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"../Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"../Competition_data/{folder_name}/X_test.csv\",header=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(len(dataset_names))\n",
    "# print(len(X_trains))  # 49, 代表有 49 個 dataFrame (每個資料集各一個)\n",
    "# print(len(y_trains))\n",
    "# print(len(X_tests))\n",
    "# print(X_trains[0].dtypes)\n",
    "# print(y_trains[0].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_trains[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "# from pyod.models.iforest import IForest\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # Step 1: 分離數值型和類別型特徵\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])  # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])  # 類別型特徵\n",
    "\n",
    "#     # Step 2: 使用 PYOD 的 IForest 檢測初始異常值\n",
    "#     pyod_clf = IForest(contamination=0.1, random_state=42)  # 假設 10% 異常\n",
    "#     pyod_clf.fit(numerical_df.values)\n",
    "#     initial_outliers = pyod_clf.predict(numerical_df.values)  # 1 表示異常，0 表示正常\n",
    "\n",
    "#     # Step 3: 將初始檢測結果作為 XGBoost 的訓練標籤\n",
    "#     xgb_clf = XGBClassifier(\n",
    "#         n_estimators=50,\n",
    "#         random_state=42,\n",
    "\n",
    "#         eval_metric=\"logloss\",\n",
    "#     )\n",
    "#     xgb_clf.fit(numerical_df, initial_outliers)\n",
    "\n",
    "#     # Step 4: 使用 XGBoost 預測訓練數據中的異常值\n",
    "#     final_outliers = xgb_clf.predict(numerical_df)\n",
    "    \n",
    "#     # Step 5: 過濾異常值\n",
    "#     is_normal = (final_outliers == 0)  # XGBoost 中 0 表示正常\n",
    "#     numerical_df = numerical_df[is_normal].reset_index(drop=True)\n",
    "#     categorical_df = categorical_df[is_normal].reset_index(drop=True)\n",
    "#     y_trains[i] = y_trains[i].iloc[is_normal].reset_index(drop=True)\n",
    "\n",
    "#     # Step 6: 對類別型特徵進行 One-Hot 編碼\n",
    "#     encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "#     categorical_encoded = encoder.fit_transform(categorical_df)\n",
    "#     categorical_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "#     # 合併處理後的數據\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "#     # Step 7: 對測試數據進行相同處理\n",
    "#     numerical_df_test = X_tests[i].select_dtypes(include=['float'])\n",
    "#     categorical_df_test = X_tests[i].select_dtypes(include=['int'])\n",
    "\n",
    "#     # 使用 XGBoost 預測測試數據中的異常值\n",
    "#     test_outliers = xgb_clf.predict(numerical_df_test)\n",
    "#     is_test_normal = (test_outliers == 0)\n",
    "#     numerical_df_test = numerical_df_test[is_test_normal].reset_index(drop=True)\n",
    "#     categorical_df_test = categorical_df_test[is_test_normal].reset_index(drop=True)\n",
    "\n",
    "#     # 使用已訓練的 One-Hot Encoder 處理類別型特徵\n",
    "#     categorical_encoded_test = encoder.transform(categorical_df_test)\n",
    "#     categorical_df_test = pd.DataFrame(categorical_encoded_test, columns=encoder.get_feature_names_out())\n",
    "\n",
    "#     # 合併測試數據\n",
    "#     X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# # 對每組資料進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 將連續型資料和數值型資料標準化\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "#         print('Splitting error')\n",
    "#     # numerical_df --> normalization\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    \n",
    "#     numerical_df = X_tests[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_tests[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     # 直接照前面用過的 scaler 來分\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_tests[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 before encoding: 20\n",
      "Dataset 0 after importance selection: 16\n",
      "Dataset 1 before encoding: 24\n",
      "Dataset 1 after importance selection: 19\n",
      "Dataset 2 before encoding: 9\n",
      "Dataset 2 after importance selection: 7\n",
      "Dataset 3 before encoding: 30\n",
      "Dataset 3 after importance selection: 24\n",
      "Dataset 4 before encoding: 19\n",
      "Dataset 4 after importance selection: 15\n",
      "Dataset 5 before encoding: 16\n",
      "Dataset 5 after importance selection: 12\n",
      "Dataset 6 before encoding: 12\n",
      "Dataset 6 after importance selection: 9\n",
      "Dataset 7 before encoding: 12\n",
      "Dataset 7 after importance selection: 9\n",
      "Dataset 8 before encoding: 46\n",
      "Dataset 8 after importance selection: 36\n",
      "Dataset 9 before encoding: 11\n",
      "Dataset 9 after importance selection: 8\n",
      "Dataset 10 before encoding: 62\n",
      "Dataset 10 after importance selection: 49\n",
      "Dataset 11 before encoding: 5\n",
      "Dataset 11 after importance selection: 4\n",
      "Dataset 12 before encoding: 54\n",
      "Dataset 12 after importance selection: 43\n",
      "Dataset 13 before encoding: 57\n",
      "Dataset 13 after importance selection: 45\n",
      "Dataset 14 before encoding: 11\n",
      "Dataset 14 after importance selection: 8\n",
      "Dataset 15 before encoding: 24\n",
      "Dataset 15 after importance selection: 19\n",
      "Dataset 16 before encoding: 24\n",
      "Dataset 16 after importance selection: 19\n",
      "Dataset 17 before encoding: 14\n",
      "Dataset 17 after importance selection: 11\n",
      "Dataset 18 before encoding: 30\n",
      "Dataset 18 after importance selection: 24\n",
      "Dataset 19 before encoding: 13\n",
      "Dataset 19 after importance selection: 10\n",
      "Dataset 20 before encoding: 17\n",
      "Dataset 20 after importance selection: 13\n",
      "Dataset 21 before encoding: 9\n",
      "Dataset 21 after importance selection: 7\n",
      "Dataset 22 before encoding: 15\n",
      "Dataset 22 after importance selection: 12\n",
      "Dataset 23 before encoding: 11\n",
      "Dataset 23 after importance selection: 8\n",
      "Dataset 24 before encoding: 6\n",
      "Dataset 24 after importance selection: 4\n",
      "Dataset 25 before encoding: 27\n",
      "Dataset 25 after importance selection: 21\n",
      "Dataset 26 before encoding: 6\n",
      "Dataset 26 after importance selection: 4\n",
      "Dataset 27 before encoding: 8\n",
      "Dataset 27 after importance selection: 6\n",
      "Dataset 28 before encoding: 13\n",
      "Dataset 28 after importance selection: 10\n",
      "Dataset 29 before encoding: 35\n",
      "Dataset 29 after importance selection: 28\n",
      "Dataset 30 before encoding: 15\n",
      "Dataset 30 after importance selection: 12\n",
      "Dataset 31 before encoding: 4\n",
      "Dataset 31 after importance selection: 3\n",
      "Dataset 32 before encoding: 13\n",
      "Dataset 32 after importance selection: 10\n",
      "Dataset 33 before encoding: 13\n",
      "Dataset 33 after importance selection: 10\n",
      "Dataset 34 before encoding: 8\n",
      "Dataset 34 after importance selection: 6\n",
      "Dataset 35 before encoding: 97\n",
      "Dataset 35 after importance selection: 77\n",
      "Dataset 36 before encoding: 13\n",
      "Dataset 36 after importance selection: 10\n",
      "Dataset 37 before encoding: 11\n",
      "Dataset 37 after importance selection: 8\n",
      "Dataset 38 before encoding: 18\n",
      "Dataset 38 after importance selection: 14\n",
      "Dataset 39 before encoding: 8\n",
      "Dataset 39 after importance selection: 6\n",
      "Dataset 40 before encoding: 10\n",
      "Dataset 40 after importance selection: 8\n",
      "Dataset 41 before encoding: 6\n",
      "Dataset 41 after importance selection: 4\n",
      "Dataset 42 before encoding: 4\n",
      "Dataset 42 after importance selection: 3\n",
      "Dataset 43 before encoding: 6\n",
      "Dataset 43 after importance selection: 4\n",
      "Dataset 44 before encoding: 4\n",
      "Dataset 44 after importance selection: 3\n",
      "Dataset 45 before encoding: 13\n",
      "Dataset 45 after importance selection: 10\n",
      "Dataset 46 before encoding: 76\n",
      "Dataset 46 after importance selection: 60\n",
      "Dataset 47 before encoding: 29\n",
      "Dataset 47 after importance selection: 23\n",
      "Dataset 48 before encoding: 44\n",
      "Dataset 48 after importance selection: 35\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# from xgboost import XGBClassifier\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# importance_selection = 0.8  # 選取重要特徵的比例\n",
    "# # 遍歷每個數據集進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     print(f\"Dataset {i} before encoding: {X_trains[i].shape[1]}\")\n",
    "    \n",
    "#     # 分離數值型和類別型特徵\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])  # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])  # 類別型特徵\n",
    "    \n",
    "#     # # One-hot encoding 類別型特徵\n",
    "#     # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "#     # categorical_df = pd.DataFrame(encoder.fit_transform(categorical_df), columns=encoder.get_feature_names_out())\n",
    "    \n",
    "#     # # 合併處理後的數據\n",
    "#     # X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "#     # # 處理測試數據集中的數值型和類別型特徵\n",
    "#     # numerical_df_test = X_tests[i].select_dtypes(include=['float'])\n",
    "#     # categorical_df_test = X_tests[i].select_dtypes(include=['int'])\n",
    "#     # categorical_df_test = pd.DataFrame(encoder.transform(categorical_df_test), columns=encoder.get_feature_names_out())\n",
    "#     # X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)\n",
    "    \n",
    "#     # 使用 XGBoost 選取重要特徵\n",
    "#     xgb_clf = XGBClassifier(\n",
    "#         n_estimators=100,\n",
    "#         random_state=42,\n",
    "#         eval_metric='auc'\n",
    "#     )\n",
    "#     xgb_clf.fit(X_trains[i], y_trains[i])\n",
    "    \n",
    "#     # 提取特徵重要性\n",
    "#     importance = xgb_clf.feature_importances_\n",
    "#     sorted_indices = importance.argsort()[::-1]\n",
    "#     top_features = sorted_indices[:int(X_trains[i].shape[1] * importance_selection)]\n",
    "    \n",
    "#     # 過濾數據，只保留重要特徵\n",
    "#     X_trains[i] = X_trains[i].iloc[:, top_features]\n",
    "#     X_tests[i] = X_tests[i].iloc[:, top_features]\n",
    "#     print(f\"Dataset {i} after importance selection: {X_trains[i].shape[1]}\")\n",
    "    \n",
    "#     # # 數值型特徵標準化\n",
    "#     # scaler = MinMaxScaler()\n",
    "#     # X_trains[i] = pd.DataFrame(scaler.fit_transform(X_trains[i]), columns=X_trains[i].columns)\n",
    "#     # X_tests[i] = pd.DataFrame(scaler.transform(X_tests[i]), columns=X_tests[i].columns)\n",
    "    \n",
    "#     # # 使用 PCA 對重要特徵進行降維\n",
    "#     # pca = PCA(n_components=0.95, random_state=42)  # 保留 95% 累積方差\n",
    "#     # X_trains[i] = pd.DataFrame(pca.fit_transform(X_trains[i]))\n",
    "#     # X_tests[i] = pd.DataFrame(pca.transform(X_tests[i]))\n",
    "#     # print(f\"PCA reduced features of Dataset {i} to {X_trains[i].shape[1]}\")\n",
    "\n",
    "# # 確保 y_trains 的標籤轉換為數值格式\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset_names)):\n",
    "#     missing_cols = set(X_trains[i].columns) - set(X_tests[i].columns)\n",
    "#     print(missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class YourTorchModel(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(YourTorchModel, self).__init__()\n",
    "        # 定義模型層\n",
    "        self.layer0 = nn.Linear(input_size, 128)\n",
    "        self.layer1 = nn.Linear(128, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        # self.layer3 = nn.Linear(12, 8)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(8)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.act_fn(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return torch.sigmoid(x)  # Apply sigmoid activation here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dataset_1: Train Acc: 0.66970849, Test AUC: 0.80280830\n",
      "Dataset Dataset_2: Train Acc: 0.92984736, Test AUC: 0.99366472\n",
      "Dataset Dataset_3: Train Acc: 0.72200012, Test AUC: 0.71428571\n",
      "Dataset Dataset_4: Train Acc: 0.92725891, Test AUC: 0.64089347\n",
      "Dataset Dataset_5: Train Acc: 0.86858177, Test AUC: 0.81147541\n",
      "Dataset Dataset_6: Train Acc: 0.88998628, Test AUC: 1.00000000\n",
      "Dataset Dataset_7: Train Acc: 0.91897207, Test AUC: 0.87656250\n",
      "Dataset Dataset_8: Train Acc: 0.75533777, Test AUC: 0.81597222\n",
      "Dataset Dataset_9: Train Acc: 0.79602915, Test AUC: 0.89523810\n",
      "Dataset Dataset_10: Train Acc: 0.62875777, Test AUC: 0.64801865\n",
      "Dataset Dataset_11: Train Acc: 0.86253685, Test AUC: 0.75000000\n",
      "Dataset Dataset_12: Train Acc: 0.72742558, Test AUC: 0.64646465\n",
      "Dataset Dataset_13: Train Acc: 0.87562472, Test AUC: 0.86013986\n",
      "Dataset Dataset_14: Train Acc: 0.91094935, Test AUC: 1.00000000\n",
      "Dataset Dataset_15: Train Acc: 0.72751677, Test AUC: 0.66833333\n",
      "Dataset Dataset_16: Train Acc: 0.93708795, Test AUC: 0.99317739\n",
      "Dataset Dataset_17: Train Acc: 0.84232640, Test AUC: 1.00000000\n",
      "Dataset Dataset_18: Train Acc: 0.92403036, Test AUC: 0.98828125\n",
      "Dataset Dataset_19: Train Acc: 0.91623873, Test AUC: 0.99470899\n",
      "Dataset Dataset_20: Train Acc: 0.80579257, Test AUC: 0.97916667\n",
      "Dataset Dataset_21: Train Acc: 0.77593642, Test AUC: 0.98131868\n",
      "Dataset Dataset_22: Train Acc: 0.76500034, Test AUC: 0.92500000\n",
      "Dataset Dataset_23: Train Acc: 0.81567758, Test AUC: 0.96885522\n",
      "Dataset Dataset_24: Train Acc: 0.67079842, Test AUC: 0.72033613\n",
      "Dataset Dataset_25: Train Acc: 0.77833372, Test AUC: 0.90588235\n",
      "Dataset Dataset_26: Train Acc: 0.75958073, Test AUC: 0.82274247\n",
      "Dataset Dataset_27: Train Acc: 0.94514143, Test AUC: 0.99771167\n",
      "Dataset Dataset_28: Train Acc: 0.72528994, Test AUC: 0.75104167\n",
      "Dataset Dataset_29: Train Acc: 0.81776005, Test AUC: 0.81693122\n",
      "Dataset Dataset_30: Train Acc: 0.72677940, Test AUC: 0.69929577\n",
      "Dataset Dataset_31: Train Acc: 0.80401045, Test AUC: 0.70000000\n",
      "Dataset Dataset_32: Train Acc: 0.84051514, Test AUC: 0.82208766\n",
      "Dataset Dataset_33: Train Acc: 0.91637158, Test AUC: 0.99814815\n",
      "Dataset Dataset_34: Train Acc: 0.73827946, Test AUC: 0.90760870\n",
      "Dataset Dataset_35: Train Acc: 0.71168464, Test AUC: 0.71536458\n",
      "Dataset Dataset_36: Train Acc: 0.84199113, Test AUC: 0.95833333\n",
      "Dataset Dataset_37: Train Acc: 0.80872339, Test AUC: 0.89164087\n",
      "Dataset Dataset_38: Train Acc: 0.68318969, Test AUC: 0.67300000\n",
      "Dataset Dataset_39: Train Acc: 0.88220805, Test AUC: 0.99166427\n",
      "Dataset Dataset_40: Train Acc: 0.70097810, Test AUC: 0.69895833\n",
      "Dataset Dataset_41: Train Acc: 0.85807312, Test AUC: 0.96279070\n",
      "Dataset Dataset_42: Train Acc: 0.91552085, Test AUC: 1.00000000\n",
      "Dataset Dataset_43: Train Acc: 0.83763891, Test AUC: 0.79094579\n",
      "Dataset Dataset_44: Train Acc: 0.93840301, Test AUC: 1.00000000\n",
      "Dataset Dataset_45: Train Acc: 0.89375561, Test AUC: 1.00000000\n",
      "Dataset Dataset_46: Train Acc: 0.82621413, Test AUC: 0.88961039\n",
      "Dataset Dataset_47: Train Acc: 0.83691049, Test AUC: 0.31944444\n",
      "Dataset Dataset_48: Train Acc: 0.79989344, Test AUC: 0.99629630\n",
      "Dataset Dataset_49: Train Acc: 0.85457969, Test AUC: 0.97330257\n",
      "平均 AUC: 0.8562755610319602\n"
     ]
    }
   ],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# # 根據不同數據續集訓練模型\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用 stratify 將數據分為訓練集和測試集\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "#     input_size = tmp_X_train.shape[1]\n",
    "#     # 將數據轉換為 PyTorch 張量\n",
    "#     # 將 DataFrame 轉為 numpy 陣列再轉為 PyTorch 張量\n",
    "#     # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#     tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#     tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#     tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#     tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "#     # 初始化模型、損失函數和優化器\n",
    "#     # model = YourTorchModel(input_size)      #if you don't have cuda gpu\n",
    "#     model = YourTorchModel(input_size).to(device)\n",
    "#     criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "    \n",
    "#     l2_lambda = 0.1  # L2 regularization factor\n",
    "#     l1_lambda = 0.01  # L1 regularization factor   \n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  \n",
    "\n",
    "#     # 訓練迴圈\n",
    "#     num_epochs = 300\n",
    "    \n",
    "#     train_acc = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "\n",
    "#         # Apply L1 regularization\n",
    "#         l1_penalty = sum(param.abs().sum() for param in model.parameters())\n",
    "#         loss = loss + l1_lambda * l1_penalty\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #train acc\n",
    "#         preds = (outputs > 0.5).float()  # 將概率轉換為類別標籤\n",
    "#         train_acc += (preds == tmp_y_train).float().mean()  # 計算準確率\n",
    "\n",
    "#     # 評估 - 預測測試集的概率\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy() # 概率預測值\n",
    "    \n",
    "#     # 計算 AUC\n",
    "#     # 經過資料處理後，可能會出現只有一個類別的情況，此時 AUC 會報錯，因此這裡做了一個判斷\n",
    "#     unique_classes = np.unique(tmp_y_test.cpu().numpy())\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "#         auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "\n",
    "#     # print(f\"{dataset_names[i]} AUC: {auc}\")\n",
    "#     avg_auc += auc\n",
    "#     models.append(model)\n",
    "\n",
    "#     train_acc /= num_epochs\n",
    "#     # 在每個 dataset 結束時打印損失和準確率,and test auc\n",
    "#     print(f\"Dataset {dataset_names[i]}: Train Acc: {train_acc.item():.8f}, Test AUC: {auc:.8f}\")   \n",
    "\n",
    "# print(\"平均 AUC:\", avg_auc / len(dataset_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把所有資料訓練集合後訓練(效果不是很好)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# num_epochs = 100  # 訓練迴圈放到最外層\n",
    "# # 把所有數據集放在一起訓練\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_train_acc = 0\n",
    "#     epoch_avg_auc = 0\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#     for i in range(len(dataset_names)):\n",
    "#         # 使用 stratify 將數據分為訓練集和測試集\n",
    "#         tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#             X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#         )\n",
    "#         input_size = tmp_X_train.shape[1]\n",
    "\n",
    "#         # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#         tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "#         # 初始化模型、損失函數和優化器\n",
    "#         # if epoch == 0 and i == 0:\n",
    "#         model = YourTorchModel(input_size).to(device)\n",
    "#         criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "#         l2_lambda = 0.1\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)\n",
    "\n",
    "#         # 訓練\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # 計算訓練準確率\n",
    "#         preds = (outputs > 0.5).float()\n",
    "#         train_acc = (preds == tmp_y_train).float().mean()\n",
    "#         epoch_train_acc += train_acc.item() / len(dataset_names)\n",
    "\n",
    "#         # 評估 - 預測測試集的概率\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy()\n",
    "\n",
    "#         # 計算 AUC\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "#         epoch_avg_auc += auc / len(dataset_names)\n",
    "\n",
    "#         # 保存模型（若在第一個 epoch 時，保存到模型列表中）\n",
    "#         if epoch == 0:\n",
    "#             models.append(model)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}: Avg Train Acc: {epoch_train_acc:.8f}, Avg Test AUC: {epoch_avg_auc:.8f}\")\n",
    "\n",
    "# print(\"整體平均 AUC:\", epoch_avg_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 這邊做一下 stratify\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # 構造 DMatrix\n",
    "#     dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "#     # 固定參數\n",
    "#     base_params = {\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'eta': 0.1,  # Learning rate\n",
    "#         'eval_metric': 'auc',\n",
    "#         'seed': 42\n",
    "#     }\n",
    "\n",
    "#     # 調參範圍\n",
    "#     max_depth_values = [3, 4, 5, 6, 7]\n",
    "#     min_child_weight_values = [1, 2, 3, 4, 5]\n",
    "\n",
    "#     # 保存結果\n",
    "#     cv_results = []\n",
    "\n",
    "#     # 網格搜索\n",
    "#     for max_depth in max_depth_values:\n",
    "#         for min_child_weight in min_child_weight_values:\n",
    "#             # 更新參數\n",
    "#             params = base_params.copy()\n",
    "#             params['max_depth'] = max_depth\n",
    "#             params['min_child_weight'] = min_child_weight\n",
    "\n",
    "#             # 交叉驗證\n",
    "#             result = xgb.cv(\n",
    "#                 params=params,\n",
    "#                 dtrain=dtrain,\n",
    "#                 num_boost_round=100,\n",
    "#                 nfold=5,\n",
    "#                 early_stopping_rounds=10,\n",
    "#                 seed=42,\n",
    "#                 as_pandas=True\n",
    "#             )\n",
    "\n",
    "#             # 提取最佳AUC及對應的迭代次數\n",
    "#             best_auc = result['test-auc-mean'].max()\n",
    "#             best_iteration = result['test-auc-mean'].idxmax() + 1\n",
    "#             cv_results.append({\n",
    "#                 'max_depth': max_depth,\n",
    "#                 'min_child_weight': min_child_weight,\n",
    "#                 'best_auc': best_auc,\n",
    "#                 'best_iteration': best_iteration\n",
    "#             })\n",
    "\n",
    "#     # 將結果轉為 DataFrame\n",
    "#     cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "#     # 查看結果\n",
    "#     # print(cv_results_df)\n",
    "\n",
    "#     # 找到最佳參數組合\n",
    "#     best_result = cv_results_df.loc[cv_results_df['best_auc'].idxmax()]\n",
    "#     print(f\"最佳參數組合: max_depth={best_result['max_depth']}, min_child_weight={best_result['min_child_weight']}\")\n",
    "#     print(f\"AUC: {best_result['best_auc']}, 最佳迭代次數: {best_result['best_iteration']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset 0\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   38.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 0: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 0 AUC: 0.7484737484737485\n",
      "Processing Dataset 1\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 1: {'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 0.9, 'alpha': 0.1, 'reg_lambda': 0.01}\n",
      "Dataset 1 AUC: 0.9814814814814814\n",
      "Processing Dataset 2\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   16.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 2: {'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'alpha': 0.1, 'reg_lambda': 0.01}\n",
      "Dataset 2 AUC: 0.5714285714285714\n",
      "Processing Dataset 3\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   25.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 3: {'n_estimators': 300, 'subsample': 0.7, 'colsample_bytree': 0.75, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 3 AUC: 0.6718213058419245\n",
      "Processing Dataset 4\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   22.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 4: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 4 AUC: 0.6639344262295082\n",
      "Processing Dataset 5\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 5: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 5 AUC: 0.9857894736842105\n",
      "Processing Dataset 6\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 6: {'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.9, 'alpha': 0.1, 'reg_lambda': 0.5}\n",
      "Dataset 6 AUC: 0.98125\n",
      "Processing Dataset 7\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 7: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 7 AUC: 0.7673611111111112\n",
      "Processing Dataset 8\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   20.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 8: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.75, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 8 AUC: 0.8476190476190476\n",
      "Processing Dataset 9\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   26.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 9: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 9 AUC: 0.49766899766899764\n",
      "Processing Dataset 10\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   20.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 10: {'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.9, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 10 AUC: 0.8500000000000001\n",
      "Processing Dataset 11\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   16.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 11: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 11 AUC: 0.9242424242424242\n",
      "Processing Dataset 12\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   24.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 12: {'n_estimators': 300, 'subsample': 0.7, 'colsample_bytree': 0.9, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 12 AUC: 0.8496503496503497\n",
      "Processing Dataset 13\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   22.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 13: {'n_estimators': 100, 'subsample': 0.7, 'colsample_bytree': 0.8, 'alpha': 0.5, 'reg_lambda': 0.01}\n",
      "Dataset 13 AUC: 0.965034965034965\n",
      "Processing Dataset 14\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   26.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 14: {'n_estimators': 300, 'subsample': 0.7, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 14 AUC: 0.70375\n",
      "Processing Dataset 15\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   21.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 15: {'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 0.9, 'alpha': 0.1, 'reg_lambda': 0.01}\n",
      "Dataset 15 AUC: 0.9814814814814814\n",
      "Processing Dataset 16\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   18.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 16: {'n_estimators': 200, 'subsample': 0.6, 'colsample_bytree': 0.9, 'alpha': 0.1, 'reg_lambda': 0.01}\n",
      "Dataset 16 AUC: 0.9370629370629371\n",
      "Processing Dataset 17\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   18.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 17: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.9, 'alpha': 0.1, 'reg_lambda': 0.01}\n",
      "Dataset 17 AUC: 0.8203125\n",
      "Processing Dataset 18\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   20.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 18: {'n_estimators': 200, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.1, 'reg_lambda': 0.1}\n",
      "Dataset 18 AUC: 0.9858906525573192\n",
      "Processing Dataset 19\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 19: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.6, 'alpha': 0.1, 'reg_lambda': 0.1}\n",
      "Dataset 19 AUC: 0.9166666666666667\n",
      "Processing Dataset 20\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   22.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 20: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.9, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 20 AUC: 0.9120879120879122\n",
      "Processing Dataset 21\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 21: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 21 AUC: 0.81875\n",
      "Processing Dataset 22\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   23.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 22: {'n_estimators': 300, 'subsample': 0.7, 'colsample_bytree': 0.9, 'alpha': 0.1, 'reg_lambda': 0.01}\n",
      "Dataset 22 AUC: 0.9755892255892256\n",
      "Processing Dataset 23\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   31.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 23: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 23 AUC: 0.6505882352941176\n",
      "Processing Dataset 24\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 24: {'n_estimators': 300, 'subsample': 0.7, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 24 AUC: 0.38823529411764707\n",
      "Processing Dataset 25\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   22.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 25: {'n_estimators': 200, 'subsample': 0.6, 'colsample_bytree': 0.8, 'alpha': 0.1, 'reg_lambda': 0.1}\n",
      "Dataset 25 AUC: 0.7357859531772576\n",
      "Processing Dataset 26\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   16.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 26: {'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'alpha': 0.5, 'reg_lambda': 0.1}\n",
      "Dataset 26 AUC: 0.9565217391304349\n",
      "Processing Dataset 27\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   25.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 27: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 27 AUC: 0.7427083333333334\n",
      "Processing Dataset 28\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 28: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 28 AUC: 0.8126984126984127\n",
      "Processing Dataset 29\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   41.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 29: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.9, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 29 AUC: 0.6172535211267606\n",
      "Processing Dataset 30\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 30: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.9, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 30 AUC: 0.8166666666666667\n",
      "Processing Dataset 31\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   21.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 31: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 31 AUC: 0.8105536332179931\n",
      "Processing Dataset 32\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   17.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 32: {'n_estimators': 100, 'subsample': 0.7, 'colsample_bytree': 0.8, 'alpha': 1, 'reg_lambda': 0.5}\n",
      "Dataset 32 AUC: 0.9981481481481481\n",
      "Processing Dataset 33\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   19.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 33: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 33 AUC: 0.8709239130434784\n",
      "Processing Dataset 34\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   25.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 34: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 34 AUC: 0.7859375\n",
      "Processing Dataset 35\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   34.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 35: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.65, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 35 AUC: 0.6022727272727273\n",
      "Processing Dataset 36\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 36: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 36 AUC: 0.7368421052631579\n",
      "Processing Dataset 37\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   25.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 37: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 37 AUC: 0.6099999999999999\n",
      "Processing Dataset 38\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   27.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 38: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.8, 'alpha': 0.1, 'reg_lambda': 0.1}\n",
      "Dataset 38 AUC: 0.9833285426846794\n",
      "Processing Dataset 39\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   25.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 39: {'n_estimators': 300, 'subsample': 0.8, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 39 AUC: 0.7859375\n",
      "Processing Dataset 40\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 40: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.5}\n",
      "Dataset 40 AUC: 0.9569767441860465\n",
      "Processing Dataset 41\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   16.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 41: {'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 1, 'reg_lambda': 1}\n",
      "Dataset 41 AUC: 0.9930555555555555\n",
      "Processing Dataset 42\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   21.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 42: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 42 AUC: 0.8105536332179931\n",
      "Processing Dataset 43\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   15.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 43: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 43 AUC: 0.9444444444444444\n",
      "Processing Dataset 44\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   15.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 44: {'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 44 AUC: 1.0\n",
      "Processing Dataset 45\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 45: {'n_estimators': 300, 'subsample': 0.7, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Dataset 45 AUC: 0.9172077922077922\n",
      "Processing Dataset 46\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   24.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 46: {'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.75, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 46 AUC: 0.40277777777777773\n",
      "Processing Dataset 47\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   24.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 47: {'n_estimators': 300, 'subsample': 0.6, 'colsample_bytree': 0.8, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 47 AUC: 0.8537037037037037\n",
      "Processing Dataset 48\n",
      "\n",
      "1152 trials detected for ('n_estimators', 'subsample', 'colsample_bytree', 'alpha', 'reg_lambda')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   16.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Dataset 48: {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.9, 'alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Dataset 48 AUC: 0.9383651944627555\n",
      "平均 AUC: 0.8179353948702601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   25.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from shaphypetune import BoostSearch\n",
    "import numpy as np\n",
    "\n",
    "# 超参数网格\n",
    "param_grid = {\n",
    "    # 'max_depth': [3, 6, 9],\n",
    "    # 'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "\n",
    "    # 'gamma': [0, 0.5, 1, 1.5, 2],\n",
    "    'subsample': [0.6,  0.7,0.8,1.0],\n",
    "    'colsample_bytree': [0.6, 0.65, 0.7,0.75,0.8,0.9],\n",
    "    'alpha': [0.01,0.1, 0.5, 1],\n",
    "    'reg_lambda': [0.01,0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "avg_auc = 0  # 累计 AUC\n",
    "n_datasets = len(dataset_names)\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    print(f\"Processing Dataset {i}\")\n",
    "    \n",
    "    # 划分数据集\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    \n",
    "    # 使用 BoostSearch 进行超参数优化\n",
    "    boost_search = BoostSearch(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        n_iter=10,\n",
    "        greater_is_better=True,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 寻找最佳模型\n",
    "    best_model = boost_search.fit(\n",
    "        tmp_X_train, tmp_y_train.squeeze(),\n",
    "        eval_set=[(tmp_X_test, tmp_y_test)],  # 添加评估集\n",
    "        verbose=False\n",
    "    )\n",
    "    print(f\"Best hyperparameters for Dataset {i}: {best_model.best_params_}\")\n",
    "    \n",
    "    # 获取预测概率\n",
    "    tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "    \n",
    "    # 检查类别数量\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for Dataset {i} due to single class in y_test.\")\n",
    "        auc = 0.7\n",
    "    else:\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    \n",
    "    avg_auc += auc\n",
    "    print(f\"Dataset {i} AUC: {auc}\")\n",
    "\n",
    "# 计算平均 AUC\n",
    "print(\"平均 AUC:\", avg_auc / n_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross vlidation(我的電腦的話一次最多跑3個參數about 5 min)\n",
    "### 預備JSON存CV最好參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加載現有的參數文件\n",
    "try:\n",
    "    with open('best_params.json', 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # 如果文件不存在，初始化一個空字典\n",
    "    best_params = {}\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    best_params.setdefault(f\"dataset_{i+1}\", {\"auc\": 0, \"params\": {}})\n",
    "    # 保存更新後的 JSON 文件\n",
    "with open('best_params.json', 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON file saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# JSON 文件路徑\n",
    "json_file_path = 'best_params.json'\n",
    "\n",
    "# 要刪除的鍵\n",
    "keys_to_delete = [\"max_depth\", \"min_child_weight\"]\n",
    "\n",
    "# 讀取現有 JSON 文件\n",
    "try:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{json_file_path}' not found.\")\n",
    "    best_params = {}\n",
    "\n",
    "# 遍歷每個 dataset 並刪除指定鍵\n",
    "for dataset, params in best_params.items():\n",
    "    for key in keys_to_delete:\n",
    "        if key in params:\n",
    "            del params[key]\n",
    "            print(f\"Deleted key '{key}' from {dataset}\")\n",
    "\n",
    "# 保存更新後的 JSON 文件\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)\n",
    "\n",
    "print(\"Updated JSON file saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先調所有人，把auc差的抓去底下加強訓練\n",
    "\n",
    "綠色代表這次調整有進步，紅色代表<bad auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for dataset 1: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 1 with tuned parameters: 0.7228327228327229\u001b[0m\n",
      "AUC for dataset 1 with tuned parameters: 0.7228327228327229\n",
      "Best parameters for dataset 2: {'n_estimators': 100}\n",
      "AUC for dataset 2 with tuned parameters: 0.9790448343079922\n",
      "Best parameters for dataset 3: {'n_estimators': 400}\n",
      "\u001b[31mAUC for dataset 3 with tuned parameters: 0.6326530612244898\u001b[0m\n",
      "AUC for dataset 3 with tuned parameters: 0.6326530612244898\n",
      "Best parameters for dataset 4: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 4 with tuned parameters: 0.7972508591065293\u001b[0m\n",
      "AUC for dataset 4 with tuned parameters: 0.7972508591065293\n",
      "Best parameters for dataset 5: {'n_estimators': 100}\n",
      "AUC for dataset 5 with tuned parameters: 0.8934426229508197\n",
      "Best parameters for dataset 6: {'n_estimators': 100}\n",
      "AUC for dataset 6 with tuned parameters: 1.0\n",
      "Best parameters for dataset 7: {'n_estimators': 100}\n",
      "AUC for dataset 7 with tuned parameters: 0.9953125\n",
      "Best parameters for dataset 8: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 8 with tuned parameters: 0.7413194444444444\u001b[0m\n",
      "AUC for dataset 8 with tuned parameters: 0.7413194444444444\n",
      "Best parameters for dataset 9: {'n_estimators': 100}\n",
      "AUC for dataset 9 with tuned parameters: 0.9\n",
      "Best parameters for dataset 10: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 10 with tuned parameters: 0.7557109557109558\u001b[0m\n",
      "AUC for dataset 10 with tuned parameters: 0.7557109557109558\n",
      "Best parameters for dataset 11: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 11 with tuned parameters: 0.575\u001b[0m\n",
      "AUC for dataset 11 with tuned parameters: 0.575\n",
      "Best parameters for dataset 12: {'n_estimators': 100}\n",
      "AUC for dataset 12 with tuned parameters: 0.9797979797979798\n",
      "Best parameters for dataset 13: {'n_estimators': 100}\n",
      "AUC for dataset 13 with tuned parameters: 0.9440559440559441\n",
      "Best parameters for dataset 14: {'n_estimators': 100}\n",
      "AUC for dataset 14 with tuned parameters: 0.9895104895104895\n",
      "Best parameters for dataset 15: {'n_estimators': 300}\n",
      "\u001b[31mAUC for dataset 15 with tuned parameters: 0.6812499999999998\u001b[0m\n",
      "AUC for dataset 15 with tuned parameters: 0.6812499999999998\n",
      "Best parameters for dataset 16: {'n_estimators': 100}\n",
      "AUC for dataset 16 with tuned parameters: 0.9790448343079922\n",
      "Best parameters for dataset 17: {'n_estimators': 100}\n",
      "AUC for dataset 17 with tuned parameters: 0.986013986013986\n",
      "Best parameters for dataset 18: {'n_estimators': 100}\n",
      "AUC for dataset 18 with tuned parameters: 0.81640625\n",
      "Best parameters for dataset 19: {'n_estimators': 100}\n",
      "AUC for dataset 19 with tuned parameters: 0.9973544973544974\n",
      "Best parameters for dataset 20: {'n_estimators': 100}\n",
      "AUC for dataset 20 with tuned parameters: 0.9444444444444444\n",
      "Best parameters for dataset 21: {'n_estimators': 100}\n",
      "AUC for dataset 21 with tuned parameters: 0.9510989010989012\n",
      "Best parameters for dataset 22: {'n_estimators': 100}\n",
      "AUC for dataset 22 with tuned parameters: 0.925\n",
      "Best parameters for dataset 23: {'n_estimators': 100}\n",
      "AUC for dataset 23 with tuned parameters: 0.9806397306397305\n",
      "Best parameters for dataset 24: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 24 with tuned parameters: 0.7468907563025211\u001b[0m\n",
      "AUC for dataset 24 with tuned parameters: 0.7468907563025211\n",
      "Best parameters for dataset 25: {'n_estimators': 100}\n",
      "AUC for dataset 25 with tuned parameters: 0.9411764705882353\n",
      "Best parameters for dataset 26: {'n_estimators': 100}\n",
      "AUC for dataset 26 with tuned parameters: 0.8662207357859532\n",
      "Best parameters for dataset 27: {'n_estimators': 100}\n",
      "AUC for dataset 27 with tuned parameters: 0.9954233409610984\n",
      "Best parameters for dataset 28: {'n_estimators': 200}\n",
      "\u001b[31mAUC for dataset 28 with tuned parameters: 0.7921875\u001b[0m\n",
      "AUC for dataset 28 with tuned parameters: 0.7921875\n",
      "Best parameters for dataset 29: {'n_estimators': 200}\n",
      "\u001b[32mAUC for dataset 29 with tuned parameters: 0.8687830687830688\u001b[0m\n",
      "Best parameters for dataset 30: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 30 with tuned parameters: 0.7257042253521127\u001b[0m\n",
      "AUC for dataset 30 with tuned parameters: 0.7257042253521127\n",
      "Best parameters for dataset 31: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 31 with tuned parameters: 0.6833333333333335\u001b[0m\n",
      "AUC for dataset 31 with tuned parameters: 0.6833333333333335\n",
      "Best parameters for dataset 32: {'n_estimators': 100}\n",
      "AUC for dataset 32 with tuned parameters: 0.8143021914648212\n",
      "Best parameters for dataset 33: {'n_estimators': 200}\n",
      "AUC for dataset 33 with tuned parameters: 0.9981481481481482\n",
      "Best parameters for dataset 34: {'n_estimators': 100}\n",
      "AUC for dataset 34 with tuned parameters: 0.8709239130434783\n",
      "Best parameters for dataset 35: {'n_estimators': 100}\n",
      "AUC for dataset 35 with tuned parameters: 0.8364583333333333\n",
      "Best parameters for dataset 36: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 36 with tuned parameters: 0.6439393939393939\u001b[0m\n",
      "AUC for dataset 36 with tuned parameters: 0.6439393939393939\n",
      "Best parameters for dataset 37: {'n_estimators': 100}\n",
      "AUC for dataset 37 with tuned parameters: 0.9102167182662538\n",
      "Best parameters for dataset 38: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 38 with tuned parameters: 0.694\u001b[0m\n",
      "AUC for dataset 38 with tuned parameters: 0.694\n",
      "Best parameters for dataset 39: {'n_estimators': 100}\n",
      "AUC for dataset 39 with tuned parameters: 0.9902270767461915\n",
      "Best parameters for dataset 40: {'n_estimators': 100}\n",
      "AUC for dataset 40 with tuned parameters: 0.8364583333333333\n",
      "Best parameters for dataset 41: {'n_estimators': 100}\n",
      "AUC for dataset 41 with tuned parameters: 0.936046511627907\n",
      "Best parameters for dataset 42: {'n_estimators': 100}\n",
      "AUC for dataset 42 with tuned parameters: 1.0\n",
      "Best parameters for dataset 43: {'n_estimators': 100}\n",
      "AUC for dataset 43 with tuned parameters: 0.8143021914648212\n",
      "Best parameters for dataset 44: {'n_estimators': 100}\n",
      "AUC for dataset 44 with tuned parameters: 0.962962962962963\n",
      "Best parameters for dataset 45: {'n_estimators': 100}\n",
      "AUC for dataset 45 with tuned parameters: 1.0\n",
      "Best parameters for dataset 46: {'n_estimators': 100}\n",
      "AUC for dataset 46 with tuned parameters: 0.9334415584415584\n",
      "Best parameters for dataset 47: {'n_estimators': 100}\n",
      "\u001b[31mAUC for dataset 47 with tuned parameters: 0.4722222222222222\u001b[0m\n",
      "AUC for dataset 47 with tuned parameters: 0.4722222222222222\n",
      "Best parameters for dataset 48: {'n_estimators': 500}\n",
      "\u001b[31mAUC for dataset 48 with tuned parameters: 0.7259259259259259\u001b[0m\n",
      "AUC for dataset 48 with tuned parameters: 0.7259259259259259\n",
      "Best parameters for dataset 49: {'n_estimators': 100}\n",
      "AUC for dataset 49 with tuned parameters: 0.9752801582069874\n",
      "Average AUC across all datasets: 0.8612603903680731\n",
      "models need to retrain: [0, 2, 3, 7, 9, 10, 14, 23, 27, 29, 30, 35, 37, 46, 47]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Define your parameter grid for XGBoost or any model you're tuning\n",
    "param_grid = {\n",
    "    \n",
    "    # 'learning_rate': [0.01,0.03,0.05,0.08,0.1,0.5],\n",
    "    'n_estimators': [100,200,300,400,500],\n",
    "    # 'gamma': [0, 0.5, 1, 1.5, 2],\n",
    "    # 'max_depth': [5,6, 7,9],\n",
    "    # 'max_delta_step': [0,1,2,3,4,5],\n",
    "    # 'subsample': [0.6, 0.65, 0.7,0.75,0.8,0.9],\n",
    "    # 'colsample_bytree': [0.6, 0.65, 0.7,0.75,0.8,0.9],\n",
    "    # 'alpha': [0.01,0.05,0.1, 0.5, 1,1.5],\n",
    "    # 'reg_lambda': [0.01,0.05,0.1, 0.5, 1,1.5],\n",
    "\n",
    "    # \"min_child_weight\": [1,2, 3, 5,7],\n",
    "}\n",
    "\n",
    "avg_auc = 0\n",
    "models = []\n",
    "retrain_index = []\n",
    "smote = SMOTE(random_state=42)\n",
    "# Define a threshold for bad AUC\n",
    "bad_threshold = 0.8\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    # Cross-validation with parameter tuning\n",
    "    model = XGBClassifier(\n",
    "        # n_estimators=500, \n",
    "        # gamma=1,\n",
    "        # eta=0.01,\n",
    "        # max_depth=6, \n",
    "        # subsample=0.7, \n",
    "        # colsample_bytree=0.6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {})    # Load the best parameters from the JSON file\n",
    "        # reg_lambda=1,     # L2 regularization term on weights\n",
    "        # alpha=0.1,         # L1 regularization term on weights\n",
    "        \n",
    "    )\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    # Get the best model from cross-validation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    models.append(best_model)\n",
    "    print(f\"Best parameters for dataset {i+1}: {grid_search.best_params_}\")\n",
    "    # Evaluate the best model\n",
    "    tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7\n",
    "    else:\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    avg_auc += auc\n",
    "\n",
    "    #將準確低於一定值的模型加入list 等等重新\n",
    "    if auc < bad_threshold:\n",
    "        retrain_index.append(i)\n",
    "        print(f\"\\033[31mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\") \n",
    "    \n",
    "    # 更新或追加參數(if auc better)\n",
    "    if auc > best_params.get(f\"dataset_{i+1}\", {}).get('auc', 0):\n",
    "        print(f\"\\033[32mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\")     \n",
    "        best_params[f\"dataset_{i+1}\"]['auc'] = auc\n",
    "\n",
    "        if f\"dataset_{i+1}\" in best_params:\n",
    "            best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "        else:\n",
    "            best_params[f\"dataset_{i+1}\"][\"params\"]  = grid_search.best_params_\n",
    "    # NO UPDATE\n",
    "    else:\n",
    "        print(f\"AUC for dataset {i+1} with tuned parameters: {auc}\")\n",
    "    # Save the best parameters to a JSON file\n",
    "    \n",
    "    with open('best_params.json', 'w') as file:\n",
    "        json.dump(best_params, file, indent=4)\n",
    "\n",
    "print(f\"Average AUC across all datasets: {avg_auc / len(dataset_names)}\")\n",
    "\n",
    "print(\"models need to retrain:\",retrain_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把準確率低的抓出來重新訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted key 'min_child_weight' from dataset_3\n",
      "Deleted key 'min_child_weight' from dataset_10\n",
      "Deleted key 'min_child_weight' from dataset_11\n",
      "Deleted key 'min_child_weight' from dataset_15\n",
      "Deleted key 'min_child_weight' from dataset_24\n",
      "Deleted key 'min_child_weight' from dataset_30\n",
      "Deleted key 'min_child_weight' from dataset_38\n",
      "Updated JSON file for retrain model saved.\n"
     ]
    }
   ],
   "source": [
    "#delete JSON file that auc is lower than 0.75\n",
    "\n",
    "# JSON 文件路徑\n",
    "json_file_path = 'best_params.json'\n",
    "\n",
    "# 要刪除的鍵\n",
    "keys_to_delete = [\"max_depth\", \"min_child_weight\"]\n",
    "\n",
    "# 讀取現有 JSON 文件\n",
    "try:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{json_file_path}' not found.\")\n",
    "    best_params = {}\n",
    "\n",
    "# 遍歷每個 dataset 並刪除指定鍵\n",
    "for dataset, params in best_params.items():\n",
    "    if(int(dataset.split(\"_\")[1])-1 in retrain_index):\n",
    "        for key in keys_to_delete:\n",
    "            if key in params:\n",
    "                del params[key]\n",
    "                print(f\"Deleted key '{key}' from {dataset}\")\n",
    "\n",
    "# 保存更新後的 JSON 文件\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)\n",
    "\n",
    "print(\"Updated JSON file for retrain model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 7, 9, 10, 14, 23, 29, 30, 35, 37, 46, 47]\n",
      "\u001b[32mAUC for dataset 1 with tuned parameters: 0.7448107448107448\u001b[0m\n",
      "AUC for retrain dataset 3 with tuned parameters: 0.6938775510204082\n",
      "AUC for retrain dataset 4 with tuned parameters: 0.6443298969072165\n",
      "AUC for retrain dataset 8 with tuned parameters: 0.7395833333333334\n",
      "AUC for retrain dataset 10 with tuned parameters: 0.6853146853146854\n",
      "AUC for retrain dataset 11 with tuned parameters: 0.5\n",
      "AUC for retrain dataset 15 with tuned parameters: 0.5916666666666667\n",
      "AUC for retrain dataset 24 with tuned parameters: 0.706890756302521\n",
      "AUC for retrain dataset 30 with tuned parameters: 0.6830985915492958\n",
      "\u001b[32mAUC for dataset 31 with tuned parameters: 0.7333333333333335\u001b[0m\n",
      "AUC for retrain dataset 36 with tuned parameters: 0.5946969696969697\n",
      "AUC for retrain dataset 38 with tuned parameters: 0.6359999999999999\n",
      "AUC for retrain dataset 47 with tuned parameters: 0.3888888888888889\n",
      "AUC for retrain dataset 48 with tuned parameters: 0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \n",
    "    # 'learning_rate': [0.005, 0.01,0.05,0.1,0.5],\n",
    "    'n_estimators': [100,200,300,400,500],         #500 400 is mostly good \n",
    "    # 'max_depth': [ 2,3,5,6, 7,9,10,12],\n",
    "    # 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    # 'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    # 'alpha': [0.01,0.1, 0.5,1.5,2.5,3],\n",
    "    'reg_lambda': [0.01,0.1,0,3, 0.5,2,2.5,3],\n",
    "    'gamma': [0,0.3, 0.5,0.7, 1, 1.5, 2,2.5,3],\n",
    "\n",
    "    # \"min_child_weight\": [1,2, 3,4, 5],    \n",
    "}\n",
    "print(retrain_index)\n",
    "\n",
    "for i in retrain_index:\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    model = XGBClassifier(\n",
    "    # n_estimators=500, \n",
    "    # gamma=1,\n",
    "    # eta=0.01,\n",
    "    # max_depth=6, \n",
    "    # subsample=0.7, \n",
    "    # colsample_bytree=0.6,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {})  # Load the best parameters from the JSON file\n",
    "    # reg_lambda=1,     # L2 regularization term on weights\n",
    "    # alpha=0.1,         # L1 regularization term on weights\n",
    "        \n",
    "    )\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=700, n_jobs=-1, random_state=2, max_depth=6,  \n",
    "    # )\n",
    "    # Get the best model from cross-validation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters for dataset {i+1}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluate the best model\n",
    "    tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "    # unique_classes = np.unique(tmp_y_test)\n",
    "    auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    \n",
    "    # Save the best parameters to JSON\n",
    "    # 更新或追加參數(if auc better)\n",
    "    if auc > best_params.get(f\"dataset_{i+1}\", {}).get('auc', 0):\n",
    "        print(f\"\\033[32mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\")\n",
    "        best_params[f\"dataset_{i+1}\"]['auc'] = auc\n",
    "\n",
    "        if f\"dataset_{i+1}\" in best_params:\n",
    "            best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "        else:\n",
    "            best_params[f\"dataset_{i+1}\"][\"params\"]  = grid_search.best_params_\n",
    "        #update model\n",
    "        models[i] = best_model\n",
    "    else:\n",
    "        print(f\"AUC for retrain dataset {i+1} with tuned parameters: {auc}\")\n",
    "    # Save the best parameters to a JSON file\n",
    "    \n",
    "    with open('best_params.json', 'w') as file:\n",
    "        json.dump(best_params, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc of dataset  0: \t0.8449328449328449\n",
      "auc of dataset  1: \t1.0\n",
      "auc of dataset  2: \t0.8367346938775511\n",
      "auc of dataset  3: \t0.7010309278350515\n",
      "auc of dataset  4: \t0.9836065573770492\n",
      "auc of dataset  5: \t0.9926315789473684\n",
      "auc of dataset  6: \t0.978125\n"
     ]
    }
   ],
   "source": [
    "##最後重新train model by best params\n",
    "\n",
    "models=[]\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    # 這邊做一下 stratify\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=700, n_jobs=-1, random_state=2, max_depth=max_deep,  \n",
    "    # )\n",
    "    # XGBoost (好像比較容易過擬合，適合大資料集) \n",
    "    # print(f\"dataset {i+1} params: {best_params.get(f'dataset_{i+1}', {}).get('params', {})}\")\n",
    "    model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        \n",
    "        **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {})  # Load the best parameters from the JSON file\n",
    "    )\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "    else:\n",
    "        tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc of dataset  0: \t0.8331071913161465\n",
      "auc of dataset  1: \t0.9995215311004784\n",
      "auc of dataset  2: \t0.8333333333333334\n",
      "auc of dataset  3: \t0.4408163265306122\n",
      "auc of dataset  4: \t0.9100000000000001\n",
      "auc of dataset  5: \t0.9612244897959183\n",
      "auc of dataset  6: \t0.9808102345415778\n",
      "auc of dataset  7: \t0.9381818181818182\n",
      "auc of dataset  8: \t0.8942307692307692\n",
      "auc of dataset  9: \t0.8102564102564103\n",
      "auc of dataset 10: \t0.28571428571428575\n",
      "auc of dataset 11: \t0.9904761904761904\n",
      "auc of dataset 12: \t0.95\n",
      "auc of dataset 13: \t1.0\n",
      "auc of dataset 14: \t0.7525000000000001\n",
      "auc of dataset 15: \t0.9995215311004784\n",
      "auc of dataset 16: \t0.9259259259259259\n",
      "auc of dataset 17: \t1.0\n",
      "auc of dataset 18: \t0.9929453262786596\n",
      "auc of dataset 19: \t0.9333333333333333\n",
      "auc of dataset 20: \t0.9953379953379954\n",
      "auc of dataset 21: \t0.8562091503267973\n",
      "auc of dataset 22: \t0.9512411347517731\n",
      "auc of dataset 23: \t0.5872093023255814\n",
      "auc of dataset 24: \t1.0\n",
      "auc of dataset 25: \t0.8499999999999999\n",
      "auc of dataset 26: \t1.0\n",
      "auc of dataset 27: \t0.8408602150537635\n",
      "auc of dataset 28: \t0.9524324324324325\n",
      "auc of dataset 29: \t0.7930800542740841\n",
      "auc of dataset 30: \t0.8333333333333333\n",
      "auc of dataset 31: \t0.8236574746008708\n",
      "auc of dataset 32: \t1.0\n",
      "auc of dataset 33: \t0.8585714285714285\n",
      "auc of dataset 34: \t0.8827956989247312\n",
      "auc of dataset 35: \t1.0\n",
      "auc of dataset 36: \t0.9104938271604939\n",
      "auc of dataset 37: \t0.74\n",
      "auc of dataset 38: \t0.9936342592592593\n",
      "auc of dataset 39: \t0.8827956989247312\n",
      "auc of dataset 40: \t0.9925857275254866\n",
      "auc of dataset 41: \t1.0\n",
      "auc of dataset 42: \t0.8236574746008708\n",
      "auc of dataset 43: \t1.0\n",
      "auc of dataset 44: \t1.0\n",
      "auc of dataset 45: \t0.9396825396825397\n",
      "auc of dataset 46: \t0.75\n",
      "auc of dataset 47: \t1.0\n",
      "auc of dataset 48: \t0.988793671720501\n",
      "avg auc :   0.89139326767189\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# 這邊用迴圈跑所有的資料集，並且將每個資料集的資料分成訓練集和測試集\n",
    "# 並且用 Random Forest 來做分類\n",
    "# 得到每個資料集的 AUC\n",
    "# for max_deep in max_deeps:\n",
    "max_deep  = 5\n",
    "models=[]\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "smote = SMOTE(random_state=42)\n",
    "for i in range(len(dataset_names)):\n",
    "    # 這邊做一下 stratify\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=700, n_jobs=-1, random_state=2, max_depth=max_deep,  \n",
    "    # )\n",
    "    # XGBoost (好像比較容易過擬合，適合大資料集) \n",
    "   \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=700, \n",
    "        # early_stopping_rounds=100,\n",
    "        eta=0.01,\n",
    "        \n",
    "        gamma=1,    # 1 is best\n",
    "        # min_child_weight = 2,\n",
    "        scale_pos_weight=2,\n",
    "        max_depth=10, \n",
    "        subsample=0.7, \n",
    "        colsample_bytree=0.6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        reg_lambda=1,     # L2 regularization term on weights\n",
    "        alpha=0.1         # L1 regularization term on weights\n",
    "    )\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "    else:\n",
    "        tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #給neural network 用\n",
    "# y_predicts = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # Get the test data and convert to tensor if needed\n",
    "#     X_test = torch.tensor(X_tests[i].values, dtype=torch.float32).to(device)  # Move to the same device as the model\n",
    "\n",
    "#     # Put model in evaluation mode\n",
    "#     models[i].eval()\n",
    "    \n",
    "#     # Disable gradient calculation for inference\n",
    "#     with torch.no_grad():\n",
    "#         # Pass data through the model and apply sigmoid to get probabilities\n",
    "#         logits = models[i](X_test)  # logits will be on the same device as the model\n",
    "#         y_predict_proba = torch.sigmoid(logits).cpu().numpy().flatten()  # Convert to numpy array\n",
    "\n",
    "#     # Store the predictions as a DataFrame\n",
    "#     df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "#     y_predicts.append(df)\n",
    "#     #print auc\n",
    "#     # print(f\"Dataset {dataset_names[i]}: Test AUC: {roc_auc_score(y_trains[i], y_predict_proba)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "##給xgboost用\n",
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    # print(X_tests[i])\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'../Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# test_aucs = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用模型進行預測，獲得類別 1 的預測機率\n",
    "#     y_predict_proba = models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    \n",
    "#     # 計算 AUC 分數\n",
    "#     auc = roc_auc_score(y_tests[i], y_predict_proba)\n",
    "#     print(f'AUC of dataset {i:2}: \\t{auc}')\n",
    "    \n",
    "#     test_aucs.append(auc)\n",
    "\n",
    "# # 平均 AUC\n",
    "# avg_test_auc = sum(test_aucs) / len(test_aucs)\n",
    "# print(\"\\nAverage Test AUC:\", avg_test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
