{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "# 收集所有資料夾名稱，按照數字順序排序資料夾名稱\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "dataset_names = sorted(dataset_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for folder_name in dataset_names:\n",
    "    # print(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(len(dataset_names))\n",
    "# print(len(X_trains))  # 49, 代表有 49 個 dataFrame (每個資料集各一個)\n",
    "# print(len(y_trains))\n",
    "# print(len(X_tests))\n",
    "# print(X_trains[0].dtypes)\n",
    "# print(y_trains[0].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_trains[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Course\\DataScience\\testenv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyod.models.iforest import IForest\n",
    "import pandas as pd\n",
    "\n",
    "# 對每組資料進行異常值檢測和標準化處理\n",
    "for i in range(len(dataset_names)):\n",
    "    # 分離數值型和類別型特徵\n",
    "    numerical_df = X_trains[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "    categorical_df = X_trains[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "    \n",
    "    # 檢查分離後的欄位數是否正確\n",
    "    if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "        print('Splitting error')\n",
    "\n",
    "    # 1. 使用 PYOD 檢測並去除異常值\n",
    "    clf = IForest(contamination=0.1)  # 設置 contamination 以調整異常值比例\n",
    "    clf.fit(numerical_df.values)\n",
    "    outliers = clf.predict(numerical_df.values)  # 1 表示異常，0 表示正常\n",
    "    # 重新篩選正常數據\n",
    "    is_normal = (outliers == 0)\n",
    "    numerical_df = numerical_df[is_normal].reset_index(drop=True)\n",
    "    categorical_df = categorical_df.iloc[is_normal].reset_index(drop=True)\n",
    "    y_trains[i] = y_trains[i].iloc[is_normal].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # 2. 對數值型特徵進行標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(numerical_df.values)  # 使用訓練數據計算均值和標準差\n",
    "    numerical_s = scaler.transform(numerical_df)\n",
    "    numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "\n",
    "    # 合併數據\n",
    "    X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    # 處理測試數據\n",
    "    numerical_df_test = X_tests[i].select_dtypes(include=['float'])   # 測試數據中的數值型特徵\n",
    "    categorical_df_test = X_tests[i].select_dtypes(include=['int'])   # 測試數據中的類別特徵\n",
    "    \n",
    "    # 測試數據使用相同的 scaler 進行標準化\n",
    "    numerical_s_test = scaler.transform(numerical_df_test)\n",
    "    numerical_df_test = pd.DataFrame(numerical_s_test, columns=numerical_df_test.columns)\n",
    "    X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)\n",
    "\n",
    "# 轉換 y_trains 標籤為數值格式\n",
    "for i in range(len(dataset_names)):\n",
    "    y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# # 對每組資料進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 將連續型資料和數值型資料標準化\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "#         print('Splitting error')\n",
    "#     # numerical_df --> normalization\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    \n",
    "#     numerical_df = X_tests[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_tests[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     # 直接照前面用過的 scaler 來分\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_tests[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# def preprocess_data(df, scaler=None, label_binarizers=None, columns=None):\n",
    "#     # 將連續型資料和數值型資料標準化\n",
    "#     numerical_df = df.select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = df.select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "\n",
    "#     # numerical_df --> normalization\n",
    "#     if scaler is None:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "\n",
    "#     # categorial_df --> label binarizer encoding\n",
    "#     if label_binarizers is None:\n",
    "#         label_binarizers = {}\n",
    "\n",
    "#     encoded_cols = []\n",
    "#     for col in categorical_df.columns:\n",
    "#         unique_values = categorical_df[col].nunique()\n",
    "\n",
    "#         if col not in label_binarizers:\n",
    "#             c_scaler = LabelBinarizer()\n",
    "#             c_scaler.fit(categorical_df[col])\n",
    "#             label_binarizers[col] = c_scaler\n",
    "#         encoded_df = label_binarizers[col].transform(categorical_df[col])  # 轉成 ndarray\n",
    "\n",
    "#         # 如果是多類別，轉換成 DataFrame，並加上欄位名稱\n",
    "#         if encoded_df.shape[1] > 1:\n",
    "#             encoded_df = pd.DataFrame(encoded_df, columns=[f\"{col}_{cls}\" for cls in label_binarizers[col].classes_])\n",
    "#         else:\n",
    "#             encoded_df = pd.Series(encoded_df.flatten(), name=categorical_df[col].name)\n",
    "#         encoded_cols.append(encoded_df)\n",
    "\n",
    "#     encoded_df = pd.concat([categorical_df.drop(columns=categorical_df.columns)] + encoded_cols, axis=1)\n",
    "\n",
    "#     # 如果是測試資料，補齊缺少的欄位並重新排序\n",
    "#     if columns is not None:\n",
    "#         missing_cols = set(columns) - set(encoded_df.columns)\n",
    "#         for col in missing_cols:\n",
    "#             encoded_df[col] = 0  # 缺失的欄位補 0\n",
    "#         encoded_df = encoded_df[columns]  # 重新排序以匹配訓練資料的欄位順序\n",
    "\n",
    "#     # 合併數值型和類別型資料框\n",
    "#     processed_df = pd.concat([numerical_df, encoded_df], axis=1)\n",
    "#     processed_df = processed_df[columns] if columns is not None else processed_df\n",
    "\n",
    "#     return processed_df, scaler, label_binarizers\n",
    "\n",
    "# # 對每組資料進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     X_trains[i], n_scaler, label_binarizers = preprocess_data(X_trains[i])\n",
    "#     X_tests[i], _, _ = preprocess_data(X_tests[i], scaler=n_scaler, label_binarizers=label_binarizers, columns=X_trains[i].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset_names)):\n",
    "#     missing_cols = set(X_trains[i].columns) - set(X_tests[i].columns)\n",
    "#     print(missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class YourTorchModel(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(YourTorchModel, self).__init__()\n",
    "        # 定義模型層\n",
    "        self.layer0 = nn.Linear(input_size, 64)\n",
    "        self.layer1 = nn.Linear(64, 8)\n",
    "        self.layer2 = nn.Linear(16, 12)\n",
    "        self.layer3 = nn.Linear(12, 8)\n",
    "        self.out = nn.Linear(8, 1)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(64)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.bn2 = nn.BatchNorm1d(12)\n",
    "        self.bn3 = nn.BatchNorm1d(8)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.layer2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.act_fn(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.act_fn(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return torch.sigmoid(x)  # Apply sigmoid activation here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dataset_1: Train Acc: 0.74337524, Test AUC: 0.77752976\n",
      "Dataset Dataset_2: Train Acc: 0.91256452, Test AUC: 0.99805068\n",
      "Dataset Dataset_3: Train Acc: 0.77476257, Test AUC: 0.47619048\n",
      "Dataset Dataset_4: Train Acc: 0.87020797, Test AUC: 0.37359551\n",
      "Dataset Dataset_5: Train Acc: 0.91553205, Test AUC: 0.95238095\n",
      "Dataset Dataset_6: Train Acc: 0.81636906, Test AUC: 0.99744246\n",
      "Dataset Dataset_7: Train Acc: 0.59630162, Test AUC: 0.98709677\n",
      "Dataset Dataset_8: Train Acc: 0.75234377, Test AUC: 0.71304348\n",
      "Dataset Dataset_9: Train Acc: 0.79582542, Test AUC: 0.86904762\n",
      "Dataset Dataset_10: Train Acc: 0.65834361, Test AUC: 0.81580511\n",
      "Dataset Dataset_11: Train Acc: 0.82926923, Test AUC: 0.16666667\n",
      "Dataset Dataset_12: Train Acc: 0.71812117, Test AUC: 1.00000000\n",
      "Dataset Dataset_13: Train Acc: 0.89474481, Test AUC: 0.89565217\n",
      "Dataset Dataset_14: Train Acc: 0.83300120, Test AUC: 1.00000000\n",
      "Dataset Dataset_15: Train Acc: 0.73016387, Test AUC: 0.68092105\n",
      "Dataset Dataset_16: Train Acc: 0.80565852, Test AUC: 0.99480182\n",
      "Dataset Dataset_17: Train Acc: 0.72709167, Test AUC: 0.95041322\n",
      "Dataset Dataset_18: Train Acc: 0.61907119, Test AUC: 1.00000000\n",
      "Dataset Dataset_19: Train Acc: 0.92323923, Test AUC: 0.99342105\n",
      "Dataset Dataset_20: Train Acc: 0.79000038, Test AUC: 0.97478992\n",
      "Dataset Dataset_21: Train Acc: 0.83470118, Test AUC: 0.91440217\n",
      "Dataset Dataset_22: Train Acc: 0.71579665, Test AUC: 0.87301587\n",
      "Dataset Dataset_23: Train Acc: 0.73899049, Test AUC: 0.96288441\n",
      "Dataset Dataset_24: Train Acc: 0.64288574, Test AUC: 0.71428571\n",
      "Dataset Dataset_25: Train Acc: 0.69694448, Test AUC: 1.00000000\n",
      "Dataset Dataset_26: Train Acc: 0.80773437, Test AUC: 0.72727273\n",
      "Dataset Dataset_27: Train Acc: 0.89644206, Test AUC: 1.00000000\n",
      "Dataset Dataset_28: Train Acc: 0.69539618, Test AUC: 0.80129870\n",
      "Dataset Dataset_29: Train Acc: 0.82362157, Test AUC: 0.84239130\n",
      "Dataset Dataset_30: Train Acc: 0.76900941, Test AUC: 0.67390321\n",
      "Dataset Dataset_31: Train Acc: 0.76758718, Test AUC: 0.88636364\n",
      "Dataset Dataset_32: Train Acc: 0.78550124, Test AUC: 0.71515345\n",
      "Dataset Dataset_33: Train Acc: 0.87996209, Test AUC: 1.00000000\n",
      "Dataset Dataset_34: Train Acc: 0.76955795, Test AUC: 0.85795455\n",
      "Dataset Dataset_35: Train Acc: 0.73495221, Test AUC: 0.92261905\n",
      "Dataset Dataset_36: Train Acc: 0.92913985, Test AUC: 0.96818182\n",
      "Dataset Dataset_37: Train Acc: 0.82322878, Test AUC: 0.86507937\n",
      "Dataset Dataset_38: Train Acc: 0.67768943, Test AUC: 0.73580247\n",
      "Dataset Dataset_39: Train Acc: 0.88012350, Test AUC: 1.00000000\n",
      "Dataset Dataset_40: Train Acc: 0.74530488, Test AUC: 0.91025641\n",
      "Dataset Dataset_41: Train Acc: 0.57222217, Test AUC: 0.97321429\n",
      "Dataset Dataset_42: Train Acc: 0.93034983, Test AUC: 1.00000000\n",
      "Dataset Dataset_43: Train Acc: 0.76841009, Test AUC: 0.70044757\n",
      "Dataset Dataset_44: Train Acc: 0.88333458, Test AUC: 1.00000000\n",
      "Dataset Dataset_45: Train Acc: 0.78430593, Test AUC: 1.00000000\n",
      "Dataset Dataset_46: Train Acc: 0.84272385, Test AUC: 0.77916667\n",
      "Dataset Dataset_47: Train Acc: 0.88879812, Test AUC: 0.50909091\n",
      "Dataset Dataset_48: Train Acc: 0.84276384, Test AUC: 0.98768473\n",
      "Dataset Dataset_49: Train Acc: 0.83626294, Test AUC: 0.99435256\n",
      "平均 AUC: 0.8557483735367474\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "avg_auc = 0\n",
    "# 根據不同數據續集訓練模型\n",
    "for i in range(len(dataset_names)):\n",
    "    # 使用 stratify 將數據分為訓練集和測試集\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "    input_size = tmp_X_train.shape[1]\n",
    "    # 將數據轉換為 PyTorch 張量\n",
    "    # 將 DataFrame 轉為 numpy 陣列再轉為 PyTorch 張量\n",
    "    # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "    tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "    tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "    tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "    tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    # 初始化模型、損失函數和優化器\n",
    "    # model = YourTorchModel(input_size)      #if you don't have cuda gpu\n",
    "    model = YourTorchModel(input_size).to(device)\n",
    "    criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "    \n",
    "    l2_lambda = 0.1  # L2 regularization factor\n",
    "    l1_lambda = 0.001  # L1 regularization factor   \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  \n",
    "\n",
    "    # 訓練迴圈\n",
    "    num_epochs = 300\n",
    "    \n",
    "    train_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tmp_X_train)\n",
    "        loss = criterion(outputs, tmp_y_train)\n",
    "\n",
    "        # Apply L1 regularization\n",
    "        l1_penalty = sum(param.abs().sum() for param in model.parameters())\n",
    "        loss = loss + l1_lambda * l1_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train acc\n",
    "        preds = (outputs > 0.5).float()  # 將概率轉換為類別標籤\n",
    "        train_acc += (preds == tmp_y_train).float().mean()  # 計算準確率\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # 評估 - 預測測試集的概率\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy() # 概率預測值\n",
    "    \n",
    "    # 計算 AUC\n",
    "    # 經過資料處理後，可能會出現只有一個類別的情況，此時 AUC 會報錯，因此這裡做了一個判斷\n",
    "    unique_classes = np.unique(tmp_y_test.cpu().numpy())\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "    else:\n",
    "        auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "\n",
    "    # print(f\"{dataset_names[i]} AUC: {auc}\")\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "\n",
    "    train_acc /= num_epochs\n",
    "    # 在每個 dataset 結束時打印損失和準確率,and test auc\n",
    "    print(f\"Dataset {dataset_names[i]}: Train Acc: {train_acc.item():.8f}, Test AUC: {auc:.8f}\")   \n",
    "\n",
    "print(\"平均 AUC:\", avg_auc / len(dataset_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把所有資料訓練集合後訓練(效果不是很好)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# num_epochs = 100  # 訓練迴圈放到最外層\n",
    "# # 把所有數據集放在一起訓練\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_train_acc = 0\n",
    "#     epoch_avg_auc = 0\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#     for i in range(len(dataset_names)):\n",
    "#         # 使用 stratify 將數據分為訓練集和測試集\n",
    "#         tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#             X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#         )\n",
    "#         input_size = tmp_X_train.shape[1]\n",
    "\n",
    "#         # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#         tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "#         # 初始化模型、損失函數和優化器\n",
    "#         # if epoch == 0 and i == 0:\n",
    "#         model = YourTorchModel(input_size).to(device)\n",
    "#         criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "#         l2_lambda = 0.1\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)\n",
    "\n",
    "#         # 訓練\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # 計算訓練準確率\n",
    "#         preds = (outputs > 0.5).float()\n",
    "#         train_acc = (preds == tmp_y_train).float().mean()\n",
    "#         epoch_train_acc += train_acc.item() / len(dataset_names)\n",
    "\n",
    "#         # 評估 - 預測測試集的概率\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy()\n",
    "\n",
    "#         # 計算 AUC\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "#         epoch_avg_auc += auc / len(dataset_names)\n",
    "\n",
    "#         # 保存模型（若在第一個 epoch 時，保存到模型列表中）\n",
    "#         if epoch == 0:\n",
    "#             models.append(model)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}: Avg Train Acc: {epoch_train_acc:.8f}, Avg Test AUC: {epoch_avg_auc:.8f}\")\n",
    "\n",
    "# print(\"整體平均 AUC:\", epoch_avg_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross vlidation(實驗中跑不動)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Define your parameter grid for XGBoost or any model you're tuning\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 300, 750],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'gamma': [0, 0.5, 1],\n",
    "#     'subsample': [0.7, 0.9],\n",
    "#     'colsample_bytree': [0.7, 0.9],\n",
    "#     'reg_lambda': [0, 1],\n",
    "#     'alpha': [0, 0.01, 0.1]\n",
    "# }\n",
    "\n",
    "# avg_auc = 0\n",
    "# models = []\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "    \n",
    "#     # Cross-validation with parameter tuning\n",
    "#     model = XGBClassifier(objective='binary:logistic', eval_metric='auc',tree_method='gpu_hist',  # Enable GPU for XGBoost\n",
    "#     use_label_encoder=False)\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "#     grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "#     # Get the best model from cross-validation\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     models.append(best_model)\n",
    "\n",
    "#     # Evaluate the best model\n",
    "#     tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "#     unique_classes = np.unique(tmp_y_test)\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "#         auc = 0.7\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "#     avg_auc += auc\n",
    "\n",
    "#     print(f\"AUC for dataset {i+1} with tuned parameters: {auc}\")\n",
    "\n",
    "# print(f\"Average AUC across all datasets: {avg_auc / len(dataset_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 這邊用迴圈跑所有的資料集，並且將每個資料集的資料分成訓練集和測試集\n",
    "# # 並且用 Random Forest 來做分類\n",
    "# # 得到每個資料集的 AUC\n",
    "# max_deep  = 5\n",
    "# # for max_deep in max_deeps:\n",
    "# models=[]\n",
    "# avg_auc = 0\n",
    "# avg_train = 0\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 這邊做一下 stratify\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     # XGBoost (好像比較容易過擬合，適合大資料集) \n",
    "#     # model = XGBClassifier(\n",
    "#     #     n_estimators=200, learning_rate=0.1, gamma=0.7,\n",
    "#     # )\n",
    "#     model = XGBClassifier(\n",
    "#         n_estimators=750, \n",
    "#         eta=0.005,\n",
    "#         # learning_rate=0.05, \n",
    "#         gamma=1, \n",
    "#         max_depth=5, \n",
    "#         subsample=0.9, \n",
    "#         colsample_bytree=0.7,\n",
    "#         objective='binary:logistic',\n",
    "#         eval_metric='auc',\n",
    "#         reg_lambda=1,     # L2 regularization term on weights\n",
    "#         alpha=0.001          # L1 regularization term on weights\n",
    "#     )\n",
    "#     model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "#     tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "#     unique_classes = np.unique(tmp_y_test)\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "#         auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "#     # print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "#     avg_auc += auc\n",
    "#     models.append(model)\n",
    "        \n",
    "\n",
    "# # print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#給neural network 用\n",
    "y_predicts = []\n",
    "for i in range(len(dataset_names)):\n",
    "    # Get the test data and convert to tensor if needed\n",
    "    X_test = torch.tensor(X_tests[i].values, dtype=torch.float32).to(device)  # Move to the same device as the model\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    models[i].eval()\n",
    "    \n",
    "    # Disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        # Pass data through the model and apply sigmoid to get probabilities\n",
    "        logits = models[i](X_test)  # logits will be on the same device as the model\n",
    "        y_predict_proba = torch.sigmoid(logits).cpu().numpy().flatten()  # Convert to numpy array\n",
    "\n",
    "    # Store the predictions as a DataFrame\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    #print auc\n",
    "    # print(f\"Dataset {dataset_names[i]}: Test AUC: {roc_auc_score(y_trains[i], y_predict_proba)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##給xgboost用\n",
    "# y_predicts=[]\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # print(X_tests[i])\n",
    "#     y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "#     df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "#     y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# test_aucs = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用模型進行預測，獲得類別 1 的預測機率\n",
    "#     y_predict_proba = models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    \n",
    "#     # 計算 AUC 分數\n",
    "#     auc = roc_auc_score(y_tests[i], y_predict_proba)\n",
    "#     print(f'AUC of dataset {i:2}: \\t{auc}')\n",
    "    \n",
    "#     test_aucs.append(auc)\n",
    "\n",
    "# # 平均 AUC\n",
    "# avg_test_auc = sum(test_aucs) / len(test_aucs)\n",
    "# print(\"\\nAverage Test AUC:\", avg_test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
