{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "# 收集所有資料夾名稱，按照數字順序排序資料夾名稱\n",
    "for folder_name in os.listdir(\"../Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "dataset_names = sorted(dataset_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for folder_name in dataset_names:\n",
    "    # print(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"../Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"../Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"../Competition_data/{folder_name}/X_test.csv\",header=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(len(dataset_names))\n",
    "# print(len(X_trains))  # 49, 代表有 49 個 dataFrame (每個資料集各一個)\n",
    "# print(len(y_trains))\n",
    "# print(len(X_tests))\n",
    "# print(X_trains[0].dtypes)\n",
    "# print(y_trains[0].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_trains[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "# from pyod.models.iforest import IForest\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # Step 1: 分離數值型和類別型特徵\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])  # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])  # 類別型特徵\n",
    "\n",
    "#     # Step 2: 使用 PYOD 的 IForest 檢測初始異常值\n",
    "#     pyod_clf = IForest(contamination=0.1, random_state=42)  # 假設 10% 異常\n",
    "#     pyod_clf.fit(numerical_df.values)\n",
    "#     initial_outliers = pyod_clf.predict(numerical_df.values)  # 1 表示異常，0 表示正常\n",
    "\n",
    "#     # Step 3: 將初始檢測結果作為 XGBoost 的訓練標籤\n",
    "#     xgb_clf = XGBClassifier(\n",
    "#         n_estimators=50,\n",
    "#         random_state=42,\n",
    "\n",
    "#         eval_metric=\"logloss\",\n",
    "#     )\n",
    "#     xgb_clf.fit(numerical_df, initial_outliers)\n",
    "\n",
    "#     # Step 4: 使用 XGBoost 預測訓練數據中的異常值\n",
    "#     final_outliers = xgb_clf.predict(numerical_df)\n",
    "    \n",
    "#     # Step 5: 過濾異常值\n",
    "#     is_normal = (final_outliers == 0)  # XGBoost 中 0 表示正常\n",
    "#     numerical_df = numerical_df[is_normal].reset_index(drop=True)\n",
    "#     categorical_df = categorical_df[is_normal].reset_index(drop=True)\n",
    "#     y_trains[i] = y_trains[i].iloc[is_normal].reset_index(drop=True)\n",
    "\n",
    "#     # Step 6: 對類別型特徵進行 One-Hot 編碼\n",
    "#     encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "#     categorical_encoded = encoder.fit_transform(categorical_df)\n",
    "#     categorical_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "#     # 合併處理後的數據\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "#     # Step 7: 對測試數據進行相同處理\n",
    "#     numerical_df_test = X_tests[i].select_dtypes(include=['float'])\n",
    "#     categorical_df_test = X_tests[i].select_dtypes(include=['int'])\n",
    "\n",
    "#     # 使用 XGBoost 預測測試數據中的異常值\n",
    "#     test_outliers = xgb_clf.predict(numerical_df_test)\n",
    "#     is_test_normal = (test_outliers == 0)\n",
    "#     numerical_df_test = numerical_df_test[is_test_normal].reset_index(drop=True)\n",
    "#     categorical_df_test = categorical_df_test[is_test_normal].reset_index(drop=True)\n",
    "\n",
    "#     # 使用已訓練的 One-Hot Encoder 處理類別型特徵\n",
    "#     categorical_encoded_test = encoder.transform(categorical_df_test)\n",
    "#     categorical_df_test = pd.DataFrame(categorical_encoded_test, columns=encoder.get_feature_names_out())\n",
    "\n",
    "#     # 合併測試數據\n",
    "#     X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# # 對每組資料進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 將連續型資料和數值型資料標準化\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "#         print('Splitting error')\n",
    "#     # numerical_df --> normalization\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    \n",
    "#     numerical_df = X_tests[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_tests[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     # 直接照前面用過的 scaler 來分\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_tests[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 before encoding: 20\n",
      "Dataset 0 after importance selection: 18\n",
      "Dataset 1 before encoding: 24\n",
      "Dataset 1 after importance selection: 21\n",
      "Dataset 2 before encoding: 9\n",
      "Dataset 2 after importance selection: 8\n",
      "Dataset 3 before encoding: 30\n",
      "Dataset 3 after importance selection: 27\n",
      "Dataset 4 before encoding: 19\n",
      "Dataset 4 after importance selection: 17\n",
      "Dataset 5 before encoding: 16\n",
      "Dataset 5 after importance selection: 14\n",
      "Dataset 6 before encoding: 12\n",
      "Dataset 6 after importance selection: 10\n",
      "Dataset 7 before encoding: 12\n",
      "Dataset 7 after importance selection: 10\n",
      "Dataset 8 before encoding: 46\n",
      "Dataset 8 after importance selection: 41\n",
      "Dataset 9 before encoding: 11\n",
      "Dataset 9 after importance selection: 9\n",
      "Dataset 10 before encoding: 62\n",
      "Dataset 10 after importance selection: 55\n",
      "Dataset 11 before encoding: 5\n",
      "Dataset 11 after importance selection: 4\n",
      "Dataset 12 before encoding: 54\n",
      "Dataset 12 after importance selection: 48\n",
      "Dataset 13 before encoding: 57\n",
      "Dataset 13 after importance selection: 51\n",
      "Dataset 14 before encoding: 11\n",
      "Dataset 14 after importance selection: 9\n",
      "Dataset 15 before encoding: 24\n",
      "Dataset 15 after importance selection: 21\n",
      "Dataset 16 before encoding: 24\n",
      "Dataset 16 after importance selection: 21\n",
      "Dataset 17 before encoding: 14\n",
      "Dataset 17 after importance selection: 12\n",
      "Dataset 18 before encoding: 30\n",
      "Dataset 18 after importance selection: 27\n",
      "Dataset 19 before encoding: 13\n",
      "Dataset 19 after importance selection: 11\n",
      "Dataset 20 before encoding: 17\n",
      "Dataset 20 after importance selection: 15\n",
      "Dataset 21 before encoding: 9\n",
      "Dataset 21 after importance selection: 8\n",
      "Dataset 22 before encoding: 15\n",
      "Dataset 22 after importance selection: 13\n",
      "Dataset 23 before encoding: 11\n",
      "Dataset 23 after importance selection: 9\n",
      "Dataset 24 before encoding: 6\n",
      "Dataset 24 after importance selection: 5\n",
      "Dataset 25 before encoding: 27\n",
      "Dataset 25 after importance selection: 24\n",
      "Dataset 26 before encoding: 6\n",
      "Dataset 26 after importance selection: 5\n",
      "Dataset 27 before encoding: 8\n",
      "Dataset 27 after importance selection: 7\n",
      "Dataset 28 before encoding: 13\n",
      "Dataset 28 after importance selection: 11\n",
      "Dataset 29 before encoding: 35\n",
      "Dataset 29 after importance selection: 31\n",
      "Dataset 30 before encoding: 15\n",
      "Dataset 30 after importance selection: 13\n",
      "Dataset 31 before encoding: 4\n",
      "Dataset 31 after importance selection: 3\n",
      "Dataset 32 before encoding: 13\n",
      "Dataset 32 after importance selection: 11\n",
      "Dataset 33 before encoding: 13\n",
      "Dataset 33 after importance selection: 11\n",
      "Dataset 34 before encoding: 8\n",
      "Dataset 34 after importance selection: 7\n",
      "Dataset 35 before encoding: 97\n",
      "Dataset 35 after importance selection: 87\n",
      "Dataset 36 before encoding: 13\n",
      "Dataset 36 after importance selection: 11\n",
      "Dataset 37 before encoding: 11\n",
      "Dataset 37 after importance selection: 9\n",
      "Dataset 38 before encoding: 18\n",
      "Dataset 38 after importance selection: 16\n",
      "Dataset 39 before encoding: 8\n",
      "Dataset 39 after importance selection: 7\n",
      "Dataset 40 before encoding: 10\n",
      "Dataset 40 after importance selection: 9\n",
      "Dataset 41 before encoding: 6\n",
      "Dataset 41 after importance selection: 5\n",
      "Dataset 42 before encoding: 4\n",
      "Dataset 42 after importance selection: 3\n",
      "Dataset 43 before encoding: 6\n",
      "Dataset 43 after importance selection: 5\n",
      "Dataset 44 before encoding: 4\n",
      "Dataset 44 after importance selection: 3\n",
      "Dataset 45 before encoding: 13\n",
      "Dataset 45 after importance selection: 11\n",
      "Dataset 46 before encoding: 76\n",
      "Dataset 46 after importance selection: 68\n",
      "Dataset 47 before encoding: 29\n",
      "Dataset 47 after importance selection: 26\n",
      "Dataset 48 before encoding: 44\n",
      "Dataset 48 after importance selection: 39\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# from xgboost import XGBClassifier\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# importance_selection = 0.9  # 選取重要特徵的比例\n",
    "# # 遍歷每個數據集進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     print(f\"Dataset {i} before encoding: {X_trains[i].shape[1]}\")\n",
    "    \n",
    "#     # 分離數值型和類別型特徵\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])  # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])  # 類別型特徵\n",
    "    \n",
    "#     # # One-hot encoding 類別型特徵\n",
    "#     # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "#     # categorical_df = pd.DataFrame(encoder.fit_transform(categorical_df), columns=encoder.get_feature_names_out())\n",
    "    \n",
    "#     # # 合併處理後的數據\n",
    "#     # X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "#     # # 處理測試數據集中的數值型和類別型特徵\n",
    "#     # numerical_df_test = X_tests[i].select_dtypes(include=['float'])\n",
    "#     # categorical_df_test = X_tests[i].select_dtypes(include=['int'])\n",
    "#     # categorical_df_test = pd.DataFrame(encoder.transform(categorical_df_test), columns=encoder.get_feature_names_out())\n",
    "#     # X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)\n",
    "    \n",
    "#     # # 數值型特徵標準化\n",
    "#     scaler = MinMaxScaler()\n",
    "#     X_trains[i] = pd.DataFrame(scaler.fit_transform(X_trains[i]), columns=X_trains[i].columns)\n",
    "#     X_tests[i] = pd.DataFrame(scaler.transform(X_tests[i]), columns=X_tests[i].columns)\n",
    "    \n",
    "#     # 使用 XGBoost 選取重要特徵\n",
    "#     xgb_clf = XGBClassifier(\n",
    "#         n_estimators=100,\n",
    "#         random_state=42,\n",
    "#         eval_metric='auc'\n",
    "#     )\n",
    "#     xgb_clf.fit(X_trains[i], y_trains[i])\n",
    "    \n",
    "#     # 提取特徵重要性\n",
    "#     importance = xgb_clf.feature_importances_\n",
    "#     sorted_indices = importance.argsort()[::-1]\n",
    "#     # GET Feature > 0.1\n",
    "#     # top_features = sorted_indices>0.1\n",
    "#     top_features = sorted_indices[:int(X_trains[i].shape[1] * importance_selection)]\n",
    "    \n",
    "#     # 過濾數據，只保留重要特徵\n",
    "#     X_trains[i] = X_trains[i].iloc[:, top_features]\n",
    "#     X_tests[i] = X_tests[i].iloc[:, top_features]\n",
    "#     print(f\"Dataset {i} after importance selection: {X_trains[i].shape[1]}\")\n",
    "    \n",
    "#     # # 使用 PCA 對重要特徵進行降維\n",
    "#     # pca = PCA(n_components=0.95, random_state=42)  # 保留 95% 累積方差\n",
    "#     # X_trains[i] = pd.DataFrame(pca.fit_transform(X_trains[i]))\n",
    "#     # X_tests[i] = pd.DataFrame(pca.transform(X_tests[i]))\n",
    "#     # print(f\"PCA reduced features of Dataset {i} to {X_trains[i].shape[1]}\")\n",
    "\n",
    "# # 確保 y_trains 的標籤轉換為數值格式\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[301], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset_names)):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mX_trains\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# for i in range(len(dataset_names)):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shaphypetune import BoostSearch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class YourTorchModel(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(YourTorchModel, self).__init__()\n",
    "        # 定義模型層\n",
    "        self.layer0 = nn.Linear(input_size, 128)\n",
    "        self.layer1 = nn.Linear(128, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        # self.layer3 = nn.Linear(12, 8)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(8)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.act_fn(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return torch.sigmoid(x)  # Apply sigmoid activation here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# # 根據不同數據續集訓練模型\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用 stratify 將數據分為訓練集和測試集\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "#     input_size = tmp_X_train.shape[1]\n",
    "#     # 將數據轉換為 PyTorch 張量\n",
    "#     # 將 DataFrame 轉為 numpy 陣列再轉為 PyTorch 張量\n",
    "#     # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#     tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#     tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#     tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#     tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "#     # 初始化模型、損失函數和優化器\n",
    "#     # model = YourTorchModel(input_size)      #if you don't have cuda gpu\n",
    "#     model = YourTorchModel(input_size).to(device)\n",
    "#     criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "    \n",
    "#     l2_lambda = 0.1  # L2 regularization factor\n",
    "#     l1_lambda = 0.01  # L1 regularization factor   \n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  \n",
    "\n",
    "#     # 訓練迴圈\n",
    "#     num_epochs = 300\n",
    "    \n",
    "#     train_acc = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "\n",
    "#         # Apply L1 regularization\n",
    "#         l1_penalty = sum(param.abs().sum() for param in model.parameters())\n",
    "#         loss = loss + l1_lambda * l1_penalty\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #train acc\n",
    "#         preds = (outputs > 0.5).float()  # 將概率轉換為類別標籤\n",
    "#         train_acc += (preds == tmp_y_train).float().mean()  # 計算準確率\n",
    "\n",
    "#     # 評估 - 預測測試集的概率\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy() # 概率預測值\n",
    "    \n",
    "#     # 計算 AUC\n",
    "#     # 經過資料處理後，可能會出現只有一個類別的情況，此時 AUC 會報錯，因此這裡做了一個判斷\n",
    "#     unique_classes = np.unique(tmp_y_test.cpu().numpy())\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "#         auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "\n",
    "#     # print(f\"{dataset_names[i]} AUC: {auc}\")\n",
    "#     avg_auc += auc\n",
    "#     models.append(model)\n",
    "\n",
    "#     train_acc /= num_epochs\n",
    "#     # 在每個 dataset 結束時打印損失和準確率,and test auc\n",
    "#     print(f\"Dataset {dataset_names[i]}: Train Acc: {train_acc.item():.8f}, Test AUC: {auc:.8f}\")   \n",
    "\n",
    "# print(\"平均 AUC:\", avg_auc / len(dataset_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把所有資料訓練集合後訓練(效果不是很好)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# num_epochs = 100  # 訓練迴圈放到最外層\n",
    "# # 把所有數據集放在一起訓練\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_train_acc = 0\n",
    "#     epoch_avg_auc = 0\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#     for i in range(len(dataset_names)):\n",
    "#         # 使用 stratify 將數據分為訓練集和測試集\n",
    "#         tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#             X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#         )\n",
    "#         input_size = tmp_X_train.shape[1]\n",
    "\n",
    "#         # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#         tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "#         # 初始化模型、損失函數和優化器\n",
    "#         # if epoch == 0 and i == 0:\n",
    "#         model = YourTorchModel(input_size).to(device)\n",
    "#         criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "#         l2_lambda = 0.1\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)\n",
    "\n",
    "#         # 訓練\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # 計算訓練準確率\n",
    "#         preds = (outputs > 0.5).float()\n",
    "#         train_acc = (preds == tmp_y_train).float().mean()\n",
    "#         epoch_train_acc += train_acc.item() / len(dataset_names)\n",
    "\n",
    "#         # 評估 - 預測測試集的概率\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy()\n",
    "\n",
    "#         # 計算 AUC\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "#         epoch_avg_auc += auc / len(dataset_names)\n",
    "\n",
    "#         # 保存模型（若在第一個 epoch 時，保存到模型列表中）\n",
    "#         if epoch == 0:\n",
    "#             models.append(model)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}: Avg Train Acc: {epoch_train_acc:.8f}, Avg Test AUC: {epoch_avg_auc:.8f}\")\n",
    "\n",
    "# print(\"整體平均 AUC:\", epoch_avg_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 超参数网格\n",
    "# param_grid = {\n",
    "#     # 'max_depth': [3, 6, 9],\n",
    "#     # 'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'scale_pos_weight':[1,2,3],\n",
    "#     'gamma': [0, 0.5, 1, 1.5],\n",
    "#     'subsample': [0.6,  0.7,0.8,1.0],\n",
    "#     'colsample_bytree': [0.6,  0.7,0.8,0.9],\n",
    "#     'alpha': [0.01,0.1, 0.5, 1],\n",
    "#     'reg_lambda': [0.01,0.1, 0.5, 1],\n",
    "# }\n",
    "\n",
    "# avg_auc = 0  # 累计 AUC\n",
    "# n_datasets = len(dataset_names)\n",
    "\n",
    "# for i in range(n_datasets):\n",
    "#     print(f\"Processing Dataset {i}\")\n",
    "    \n",
    "#     # 划分数据集\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "    \n",
    "#     # 初始化模型\n",
    "#     xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    \n",
    "#     # 使用 BoostSearch 进行超参数优化\n",
    "#     boost_search = BoostSearch(\n",
    "#         estimator=xgb_model,\n",
    "#         param_grid=param_grid,\n",
    "#         n_iter=10,\n",
    "#         greater_is_better=True,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     # 寻找最佳模型\n",
    "#     best_model = boost_search.fit(\n",
    "#         tmp_X_train, tmp_y_train.squeeze(),\n",
    "#         eval_set=[(tmp_X_test, tmp_y_test)],  # 添加评估集\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     print(f\"Best hyperparameters for Dataset {i}: {best_model.best_params_}\")\n",
    "    \n",
    "#     # 获取预测概率\n",
    "#     tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "#     unique_classes = np.unique(tmp_y_test)\n",
    "    \n",
    "#     # 检查类别数量\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for Dataset {i} due to single class in y_test.\")\n",
    "#         auc = 0.7\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    \n",
    "#     avg_auc += auc\n",
    "#     print(f\"Dataset {i} AUC: {auc}\")\n",
    "\n",
    "# # 计算平均 AUC\n",
    "# print(\"平均 AUC:\", avg_auc / n_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross vlidation(我的電腦的話一次最多跑3個參數about 5 min)\n",
    "### 預備JSON存CV最好參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加載現有的參數文件\n",
    "try:\n",
    "    with open('best_params.json', 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # 如果文件不存在，初始化一個空字典\n",
    "    best_params = {}\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    best_params.setdefault(f\"dataset_{i+1}\", {\"auc\": 0, \"params\": {}})\n",
    "    # 保存更新後的 JSON 文件\n",
    "with open('best_params.json', 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted key 'subsample' from dataset_8\n",
      "Deleted key 'colsample_bytree' from dataset_8\n",
      "Deleted key 'subsample' from dataset_10\n",
      "Deleted key 'colsample_bytree' from dataset_10\n",
      "Deleted key 'subsample' from dataset_12\n",
      "Deleted key 'colsample_bytree' from dataset_12\n",
      "Deleted key 'subsample' from dataset_17\n",
      "Deleted key 'colsample_bytree' from dataset_17\n",
      "Deleted key 'subsample' from dataset_19\n",
      "Deleted key 'colsample_bytree' from dataset_19\n",
      "Deleted key 'subsample' from dataset_24\n",
      "Deleted key 'colsample_bytree' from dataset_24\n",
      "Deleted key 'subsample' from dataset_30\n",
      "Deleted key 'colsample_bytree' from dataset_30\n",
      "Deleted key 'subsample' from dataset_31\n",
      "Deleted key 'colsample_bytree' from dataset_31\n",
      "Deleted key 'subsample' from dataset_32\n",
      "Deleted key 'colsample_bytree' from dataset_32\n",
      "Deleted key 'subsample' from dataset_35\n",
      "Deleted key 'colsample_bytree' from dataset_35\n",
      "Deleted key 'subsample' from dataset_39\n",
      "Deleted key 'colsample_bytree' from dataset_39\n",
      "Deleted key 'subsample' from dataset_40\n",
      "Deleted key 'colsample_bytree' from dataset_40\n",
      "Deleted key 'subsample' from dataset_41\n",
      "Deleted key 'colsample_bytree' from dataset_41\n",
      "Deleted key 'subsample' from dataset_43\n",
      "Deleted key 'colsample_bytree' from dataset_43\n",
      "Deleted key 'subsample' from dataset_49\n",
      "Deleted key 'colsample_bytree' from dataset_49\n",
      "Updated JSON file saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 文件路徑\n",
    "json_file_path = 'best_params.json'\n",
    "\n",
    "# 要刪除的鍵\n",
    "keys_to_delete = [\"subsample\", \"colsample_bytree\"]\n",
    "\n",
    "# 讀取現有 JSON 文件\n",
    "try:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{json_file_path}' not found.\")\n",
    "    best_params = {}\n",
    "\n",
    "# 遍歷每個 dataset 並刪除指定鍵\n",
    "for dataset, dataset_content in best_params.items():\n",
    "    params = dataset_content.get(\"params\", {})\n",
    "    for key in keys_to_delete:\n",
    "        if key in params:\n",
    "            del params[key]\n",
    "            print(f\"Deleted key '{key}' from {dataset}\")\n",
    "\n",
    "# 保存更新後的 JSON 文件\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)\n",
    "\n",
    "print(\"Updated JSON file saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先調所有人，把auc差的抓去底下加強訓練\n",
    "\n",
    "綠色代表這次調整有進步，紅色代表<bad auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for dataset 1: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 2: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 3: {'scale_pos_weight': 5}\n",
      "\u001b[31mAUC for dataset 3 with tuned parameters: 0.7346938775510203\u001b[0m\n",
      "Best parameters for dataset 4: {'scale_pos_weight': 4}\n",
      "\u001b[31mAUC for dataset 4 with tuned parameters: 0.5790378006872852\u001b[0m\n",
      "Best parameters for dataset 5: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 6: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 7: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 8: {'scale_pos_weight': 1}\n",
      "\u001b[31mAUC for dataset 8 with tuned parameters: 0.7916666666666666\u001b[0m\n",
      "Best parameters for dataset 9: {'scale_pos_weight': 3}\n",
      "Best parameters for dataset 10: {'scale_pos_weight': 3}\n",
      "\u001b[31mAUC for dataset 10 with tuned parameters: 0.7620046620046621\u001b[0m\n",
      "Best parameters for dataset 11: {'scale_pos_weight': 1}\n",
      "\u001b[31mAUC for dataset 11 with tuned parameters: 0.7999999999999999\u001b[0m\n",
      "Best parameters for dataset 12: {'scale_pos_weight': 3}\n",
      "Best parameters for dataset 13: {'scale_pos_weight': 3}\n",
      "Best parameters for dataset 14: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 15: {'scale_pos_weight': 1}\n",
      "\u001b[31mAUC for dataset 15 with tuned parameters: 0.6108333333333332\u001b[0m\n",
      "Best parameters for dataset 16: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 17: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 18: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 19: {'scale_pos_weight': 2}\n",
      "Best parameters for dataset 20: {'scale_pos_weight': 3}\n",
      "Best parameters for dataset 21: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 22: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 23: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 24: {'scale_pos_weight': 3}\n",
      "\u001b[31mAUC for dataset 24 with tuned parameters: 0.6423529411764706\u001b[0m\n",
      "Best parameters for dataset 25: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 26: {'scale_pos_weight': 2}\n",
      "Best parameters for dataset 27: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 28: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 29: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 30: {'scale_pos_weight': 2}\n",
      "\u001b[31mAUC for dataset 30 with tuned parameters: 0.7119718309859155\u001b[0m\n",
      "Best parameters for dataset 31: {'scale_pos_weight': 5}\n",
      "\u001b[31mAUC for dataset 31 with tuned parameters: 0.65\u001b[0m\n",
      "Best parameters for dataset 32: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 33: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 34: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 35: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 36: {'scale_pos_weight': 3}\n",
      "Best parameters for dataset 37: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 38: {'scale_pos_weight': 2}\n",
      "\u001b[31mAUC for dataset 38 with tuned parameters: 0.596\u001b[0m\n",
      "Best parameters for dataset 39: {'scale_pos_weight': 3}\n",
      "Best parameters for dataset 40: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 41: {'scale_pos_weight': 5}\n",
      "Best parameters for dataset 42: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 43: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 44: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 45: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 46: {'scale_pos_weight': 4}\n",
      "Best parameters for dataset 47: {'scale_pos_weight': 5}\n",
      "\u001b[31mAUC for dataset 47 with tuned parameters: 0.6527777777777778\u001b[0m\n",
      "Best parameters for dataset 48: {'scale_pos_weight': 1}\n",
      "Best parameters for dataset 49: {'scale_pos_weight': 3}\n",
      "Average AUC across all datasets: 0.8820419438758198\n",
      "models need to retrain: [2, 3, 7, 9, 10, 14, 23, 29, 30, 37, 46]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Define your parameter grid for XGBoost or any model you're tuning\n",
    "param_grid = {\n",
    "    \n",
    "    \"scale_pos_weight\":[1,2,3,4,5],\n",
    "\n",
    "    # 'gamma': [0.8,0.85,0.95, 1, 1.5],\n",
    "    # 'max_depth': [3,5,6, 7,9],\n",
    "    # 'min_child_weight': [1,2, 3, 5,7],\n",
    "\n",
    "    # 'eta': [0.001,0.005,0.008,0.01],\n",
    "    # 'n_estimators': [100,200,300,400,500,700],\n",
    "\n",
    "    # 'subsample': [0.6, 0.65, 0.7,0.75,0.8,0.9],\n",
    "    # 'colsample_bytree': [0.6, 0.65, 0.7,0.75,0.8,0.9],\n",
    "    # 'max_delta_step': [0,1,2,3,4,5],\n",
    "    # 'alpha': [0.01,0.05,0.1, 0.5, 1,1.5],\n",
    "    # 'reg_lambda': [0.01,0.05,0.1, 0.5, 1,1.5],\n",
    "}\n",
    "\n",
    "avg_auc = 0\n",
    "models = []\n",
    "retrain_index = []\n",
    "smote = SMOTE(random_state=42)\n",
    "# Define a threshold for bad AUC\n",
    "bad_threshold = 0.8\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42,stratify=y_trains[i] \n",
    "    )\n",
    "    # tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    # Cross-validation with parameter tuning\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=700, \n",
    "        # early_stopping_rounds=100,\n",
    "        # eta=0.008,\n",
    "        \n",
    "        gamma=0.95,    # 0.95 is best\n",
    "        # min_child_weight = 2,\n",
    "        # scale_pos_weight=2,\n",
    "        max_depth=20, \n",
    "        subsample=0.7, \n",
    "        colsample_bytree=0.6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        reg_lambda=1,     # L2 regularization term on weights\n",
    "        alpha=0.1,  \n",
    "        # **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {})    # Load the best parameters from the JSON file\n",
    "        \n",
    "    )\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    # Get the best model from cross-validation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    models.append(best_model)\n",
    "    print(f\"Best parameters for dataset {i+1}: {grid_search.best_params_}\")\n",
    "    # Evaluate the best model\n",
    "    tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7\n",
    "    else:\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    avg_auc += auc\n",
    "\n",
    "    #將準確低於一定值的模型加入list 等等重新\n",
    "    if auc < bad_threshold:\n",
    "        retrain_index.append(i)\n",
    "        print(f\"\\033[31mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\") \n",
    "    best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "    # 更新或追加參數(if auc better)\n",
    "    # if auc > best_params.get(f\"dataset_{i+1}\", {}).get('auc', 0):\n",
    "    #     print(f\"\\033[32mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\")     \n",
    "    #     best_params[f\"dataset_{i+1}\"]['auc'] = auc\n",
    "\n",
    "    #     if f\"dataset_{i+1}\" in best_params:\n",
    "    #         best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "    #     else:\n",
    "    #         best_params[f\"dataset_{i+1}\"][\"params\"]  = grid_search.best_params_\n",
    "    # # NO UPDATE\n",
    "    # else:\n",
    "        # print(f\"AUC for dataset {i+1} with tuned parameters: {auc}\")\n",
    "    # Save the best parameters to a JSON file\n",
    "    \n",
    "    with open('best_params.json', 'w') as file:\n",
    "        json.dump(best_params, file, indent=4)\n",
    "\n",
    "print(f\"Average AUC across all datasets: {avg_auc / len(dataset_names)}\")\n",
    "\n",
    "print(\"models need to retrain:\",retrain_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把準確率低的抓出來重新訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON file for retrain model saved.\n"
     ]
    }
   ],
   "source": [
    "#delete JSON file that auc is lower than 0.75\n",
    "\n",
    "# JSON 文件路徑\n",
    "json_file_path = 'best_params.json'\n",
    "\n",
    "# 要刪除的鍵\n",
    "keys_to_delete = [\"subsample\", \"colsample_bytree\",]\n",
    "\n",
    "# 讀取現有 JSON 文件\n",
    "try:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{json_file_path}' not found.\")\n",
    "    best_params = {}\n",
    "\n",
    "# 遍歷每個 dataset 並刪除指定鍵\n",
    "for dataset, params in best_params.items():\n",
    "    if(int(dataset.split(\"_\")[1])-1 in retrain_index):\n",
    "        for key in keys_to_delete:\n",
    "            if key in params:\n",
    "                del params[key]\n",
    "                print(f\"Deleted key '{key}' from {dataset}\")\n",
    "\n",
    "# 保存更新後的 JSON 文件\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)\n",
    "\n",
    "print(\"Updated JSON file for retrain model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 10, 23, 46]\n",
      "Best parameters for dataset 4: {}\n",
      "AUC for retrain dataset 4 with tuned parameters: 0.463265306122449\n",
      "Best parameters for dataset 11: {}\n",
      "AUC for retrain dataset 11 with tuned parameters: 0.31428571428571433\n",
      "Best parameters for dataset 24: {}\n",
      "AUC for retrain dataset 24 with tuned parameters: 0.5950752393980848\n",
      "Best parameters for dataset 47: {}\n",
      "AUC for retrain dataset 47 with tuned parameters: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \n",
    "    # 'learning_rate': [0.005, 0.01,0.05,0.1],\n",
    "    # # 'n_estimators': [100,200,300,400,500],         #500 400 is mostly good \n",
    "    # # 'max_depth': [ 2,3,5,6, 7,9,10,12],\n",
    "    # # 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    # # 'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    # 'alpha': [0.01,0.1, 0.5,1.5,],\n",
    "    # 'reg_lambda': [0.01,0.1, 0.5,2],\n",
    "    # 'gamma': [0.1,0.8, 0.95, 1, 1.5, 2,],\n",
    "\n",
    "    # \"min_child_weight\": [1,2, 3,4, 5],    \n",
    "}\n",
    "print(retrain_index)\n",
    "\n",
    "for i in retrain_index:\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=700, \n",
    "        # early_stopping_rounds=100,\n",
    "        # eta=0.008,\n",
    "        \n",
    "        # gamma=0.95,    # 1 is best\n",
    "        # min_child_weight = 2,\n",
    "        # scale_pos_weight=2,\n",
    "        max_depth=15, \n",
    "        subsample=0.7, \n",
    "        colsample_bytree=0.6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        # reg_lambda=1,     # L2 regularization term on weights\n",
    "        # alpha=0.1         # L1 regularization term on weights\n",
    "    **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {})  # Load the best parameters from the JSON file\n",
    "    )\n",
    "        \n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=700, n_jobs=-1, random_state=2, max_depth=6,  \n",
    "    # )\n",
    "    # Get the best model from cross-validation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters for dataset {i+1}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluate the best model\n",
    "    tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "    # unique_classes = np.unique(tmp_y_test)\n",
    "    auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "    # Save the best parameters to JSON\n",
    "    # # 更新或追加參數(if auc better)\n",
    "    # if auc > best_params.get(f\"dataset_{i+1}\", {}).get('auc', 0):\n",
    "    #     print(f\"\\033[32mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\")\n",
    "    #     best_params[f\"dataset_{i+1}\"]['auc'] = auc\n",
    "\n",
    "    #     if f\"dataset_{i+1}\" in best_params:\n",
    "    #         best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "    #     else:\n",
    "    #         best_params[f\"dataset_{i+1}\"][\"params\"]  = grid_search.best_params_\n",
    "    #     #update model\n",
    "    models[i] = best_model\n",
    "    # else:\n",
    "    print(f\"AUC for retrain dataset {i+1} with tuned parameters: {auc}\")\n",
    "    # Save the best parameters to a JSON file\n",
    "    \n",
    "    with open('best_params.json', 'w') as file:\n",
    "        json.dump(best_params, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##最後重新train model by best params\n",
    "\n",
    "# 讀取現有 JSON 文件\n",
    "\n",
    "models=[]\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "  \n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    # print(f\"dataset {i+1} params: {best_params.get(f'dataset_{i+1}', {}).get('params', {})}\")\n",
    "    model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {}),  # Load the best parameters from the JSON file\n",
    "    )\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "    else:\n",
    "        tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc of dataset  0: \t0.8514246947082769\n",
      "auc of dataset  1: \t0.9995215311004785\n",
      "auc of dataset  2: \t0.7916666666666667\n",
      "auc of dataset  3: \t0.44693877551020406\n",
      "auc of dataset  4: \t0.9066666666666667\n",
      "auc of dataset  5: \t0.9551020408163265\n",
      "auc of dataset  6: \t0.9786780383795309\n",
      "auc of dataset  7: \t0.9418181818181819\n",
      "auc of dataset  8: \t0.8942307692307693\n",
      "auc of dataset  9: \t0.7934731934731934\n",
      "auc of dataset 10: \t0.2857142857142857\n",
      "auc of dataset 11: \t0.9904761904761904\n",
      "auc of dataset 12: \t0.96\n",
      "auc of dataset 13: \t1.0\n",
      "auc of dataset 14: \t0.7583333333333332\n",
      "auc of dataset 15: \t0.9995215311004785\n",
      "auc of dataset 16: \t0.9259259259259259\n",
      "auc of dataset 17: \t1.0\n",
      "auc of dataset 18: \t0.9920634920634921\n",
      "auc of dataset 19: \t0.9575757575757575\n",
      "auc of dataset 20: \t0.993006993006993\n",
      "auc of dataset 21: \t0.869281045751634\n",
      "auc of dataset 22: \t0.947695035460993\n",
      "auc of dataset 23: \t0.6251709986320109\n",
      "auc of dataset 24: \t1.0\n",
      "auc of dataset 25: \t0.8461538461538461\n",
      "auc of dataset 26: \t1.0\n",
      "auc of dataset 27: \t0.8564516129032258\n",
      "auc of dataset 28: \t0.947027027027027\n",
      "auc of dataset 29: \t0.7951153324287653\n",
      "auc of dataset 30: \t0.8636363636363635\n",
      "auc of dataset 31: \t0.8109579100145138\n",
      "auc of dataset 32: \t1.0\n",
      "auc of dataset 33: \t0.8571428571428571\n",
      "auc of dataset 34: \t0.8521505376344086\n",
      "auc of dataset 35: \t1.0\n",
      "auc of dataset 36: \t0.888888888888889\n",
      "auc of dataset 37: \t0.726\n",
      "auc of dataset 38: \t0.9930555555555556\n",
      "auc of dataset 39: \t0.8521505376344086\n",
      "auc of dataset 40: \t0.9935125115848008\n",
      "auc of dataset 41: \t1.0\n",
      "auc of dataset 42: \t0.8109579100145138\n",
      "auc of dataset 43: \t1.0\n",
      "auc of dataset 44: \t1.0\n",
      "auc of dataset 45: \t0.9523809523809523\n",
      "auc of dataset 46: \t0.6071428571428571\n",
      "auc of dataset 47: \t1.0\n",
      "auc of dataset 48: \t0.9884640738299276\n",
      "avg auc :   0.8878668147221286\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 這邊用迴圈跑所有的資料集，並且將每個資料集的資料分成訓練集和測試集\n",
    "# 並且用 Random Forest 來做分類\n",
    "# 得到每個資料集的 AUC\n",
    "# for max_deep in max_deeps:\n",
    "max_deep  = 5\n",
    "models=[]\n",
    "retrain_index = []\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "smote = SMOTE(random_state=42)\n",
    "for i in range(len(dataset_names)):\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=700, \n",
    "        # early_stopping_rounds=100,\n",
    "        eta=0.008,\n",
    "        \n",
    "        gamma=0.95,    # 1 is best\n",
    "        # min_child_weight = 2,\n",
    "        scale_pos_weight=2,\n",
    "        max_depth=15, \n",
    "        subsample=0.7, \n",
    "        colsample_bytree=0.6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        reg_lambda=1,     # L2 regularization term on weights\n",
    "        alpha=0.1         # L1 regularization term on weights\n",
    "    )\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "    else:\n",
    "        tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "    # if(auc < 0.7):\n",
    "    #     retrain_index.append(i)\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10', 'Feature_11', 'Feature_12', 'Feature_13', 'Feature_14', 'Feature_15', 'Feature_16', 'Feature_17', 'Feature_18', 'Feature_19', 'Feature_20'] ['Feature_1', 'Feature_20', 'Feature_12', 'Feature_2', 'Feature_7', 'Feature_8', 'Feature_16', 'Feature_4', 'Feature_5', 'Feature_19', 'Feature_15', 'Feature_3', 'Feature_18', 'Feature_13', 'Feature_11', 'Feature_6', 'Feature_17', 'Feature_9']\nexpected Feature_14, Feature_10 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[324], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m y_predicts\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset_names)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# print(X_tests[i])\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     y_predict_proba\u001b[38;5;241m=\u001b[39m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tests\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_predict_proba, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_predict_proba\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m     y_predicts\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[1;32md:\\Course\\DataScience\\testenv\\Lib\\site-packages\\xgboost\\sklearn.py:1644\u001b[0m, in \u001b[0;36mXGBClassifier.predict_proba\u001b[1;34m(self, X, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1642\u001b[0m     class_prob \u001b[38;5;241m=\u001b[39m softmax(raw_predt, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m class_prob\n\u001b[1;32m-> 1644\u001b[0m class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _cls_predict_proba(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_, class_probs, np\u001b[38;5;241m.\u001b[39mvstack)\n",
      "File \u001b[1;32md:\\Course\\DataScience\\testenv\\Lib\\site-packages\\xgboost\\sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32md:\\Course\\DataScience\\testenv\\Lib\\site-packages\\xgboost\\core.py:2510\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2508\u001b[0m     data, fns, _ \u001b[38;5;241m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[0;32m   2509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[1;32m-> 2510\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[0;32m   2512\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n",
      "File \u001b[1;32md:\\Course\\DataScience\\testenv\\Lib\\site-packages\\xgboost\\core.py:3075\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   3069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[0;32m   3070\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   3071\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3072\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[0;32m   3073\u001b[0m     )\n\u001b[1;32m-> 3075\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10', 'Feature_11', 'Feature_12', 'Feature_13', 'Feature_14', 'Feature_15', 'Feature_16', 'Feature_17', 'Feature_18', 'Feature_19', 'Feature_20'] ['Feature_1', 'Feature_20', 'Feature_12', 'Feature_2', 'Feature_7', 'Feature_8', 'Feature_16', 'Feature_4', 'Feature_5', 'Feature_19', 'Feature_15', 'Feature_3', 'Feature_18', 'Feature_13', 'Feature_11', 'Feature_6', 'Feature_17', 'Feature_9']\nexpected Feature_14, Feature_10 in input data"
     ]
    }
   ],
   "source": [
    "##給xgboost用\n",
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    # print(X_tests[i])\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'../Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# test_aucs = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用模型進行預測，獲得類別 1 的預測機率\n",
    "#     y_predict_proba = models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    \n",
    "#     # 計算 AUC 分數\n",
    "#     auc = roc_auc_score(y_tests[i], y_predict_proba)\n",
    "#     print(f'AUC of dataset {i:2}: \\t{auc}')\n",
    "    \n",
    "#     test_aucs.append(auc)\n",
    "\n",
    "# # 平均 AUC\n",
    "# avg_test_auc = sum(test_aucs) / len(test_aucs)\n",
    "# print(\"\\nAverage Test AUC:\", avg_test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
