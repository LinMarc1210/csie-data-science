{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "# 收集所有資料夾名稱，按照數字順序排序資料夾名稱\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "dataset_names = sorted(dataset_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for folder_name in dataset_names:\n",
    "    # print(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(len(dataset_names))\n",
    "# print(len(X_trains))  # 49, 代表有 49 個 dataFrame (每個資料集各一個)\n",
    "# print(len(y_trains))\n",
    "# print(len(X_tests))\n",
    "# print(X_trains[0].dtypes)\n",
    "# print(y_trains[0].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_trains[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "# from pyod.models.iforest import IForest\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # Step 1: 分離數值型和類別型特徵\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])  # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])  # 類別型特徵\n",
    "\n",
    "#     # Step 2: 使用 PYOD 的 IForest 檢測初始異常值\n",
    "#     pyod_clf = IForest(contamination=0.1, random_state=42)  # 假設 10% 異常\n",
    "#     pyod_clf.fit(numerical_df.values)\n",
    "#     initial_outliers = pyod_clf.predict(numerical_df.values)  # 1 表示異常，0 表示正常\n",
    "\n",
    "#     # Step 3: 將初始檢測結果作為 XGBoost 的訓練標籤\n",
    "#     xgb_clf = XGBClassifier(\n",
    "#         n_estimators=50,\n",
    "#         random_state=42,\n",
    "#         use_label_encoder=False,\n",
    "#         eval_metric=\"logloss\",\n",
    "#     )\n",
    "#     xgb_clf.fit(numerical_df, initial_outliers)\n",
    "\n",
    "#     # Step 4: 使用 XGBoost 檢測異常值\n",
    "#     final_outliers = xgb_clf.predict(numerical_df)\n",
    "\n",
    "#     # Step 5: 過濾異常值\n",
    "#     is_normal = (final_outliers == 0)  # XGBoost 中 0 表示正常\n",
    "#     numerical_df = numerical_df[is_normal].reset_index(drop=True)\n",
    "#     categorical_df = categorical_df[is_normal].reset_index(drop=True)\n",
    "#     y_trains[i] = y_trains[i].iloc[is_normal].reset_index(drop=True)\n",
    "\n",
    "#     # Step 6: 對類別型特徵進行 One-Hot 編碼\n",
    "#     encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "#     categorical_encoded = encoder.fit_transform(categorical_df)\n",
    "#     categorical_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "#     # 合併處理後的數據\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "#     # Step 7: 對測試數據進行相同處理\n",
    "#     numerical_df_test = X_tests[i].select_dtypes(include=['float'])\n",
    "#     categorical_df_test = X_tests[i].select_dtypes(include=['int'])\n",
    "\n",
    "#     # 使用 XGBoost 預測測試數據中的異常值\n",
    "#     test_outliers = xgb_clf.predict(numerical_df_test)\n",
    "#     is_test_normal = (test_outliers == 0)\n",
    "#     numerical_df_test = numerical_df_test[is_test_normal].reset_index(drop=True)\n",
    "#     categorical_df_test = categorical_df_test[is_test_normal].reset_index(drop=True)\n",
    "\n",
    "#     # 使用已訓練的 One-Hot Encoder 處理類別型特徵\n",
    "#     categorical_encoded_test = encoder.transform(categorical_df_test)\n",
    "#     categorical_df_test = pd.DataFrame(categorical_encoded_test, columns=encoder.get_feature_names_out())\n",
    "\n",
    "#     # 合併測試數據\n",
    "#     X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "\n",
    "# # 對每組資料進行處理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 將連續型資料和數值型資料標準化\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "#         print('Splitting error')\n",
    "#     # numerical_df --> normalization\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    \n",
    "#     numerical_df = X_tests[i].select_dtypes(include=['float'])   # 數值型特徵\n",
    "#     categorical_df = X_tests[i].select_dtypes(include=['int'])   # 類別型特徵（可能為類別特徵）\n",
    "#     # 直接照前面用過的 scaler 來分\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "#     X_tests[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.feature_selection import SelectKBest, chi2\n",
    "# import pandas as pd\n",
    "\n",
    "# # 对每组数据进行处理\n",
    "# for i in range(len(dataset_names)):\n",
    "#     print(\"before encoding: \", X_trains[i].shape[1])\n",
    "#     # 将连续型数据和数值型数据标准化\n",
    "#     numerical_df = X_trains[i].select_dtypes(include=['float'])   # 数值型特征\n",
    "#     categorical_df = X_trains[i].select_dtypes(include=['int'])   # 类别型特征（可能为类别特征）\n",
    "#     if len(numerical_df.columns) + len(categorical_df.columns) != len(X_trains[i].columns):\n",
    "#         print('Splitting error')\n",
    "    \n",
    "#     # 数值型特征标准化\n",
    "#     scaler = MinMaxScaler()\n",
    "#     scaler.fit(numerical_df)\n",
    "#     numerical_s = scaler.transform(numerical_df)\n",
    "#     numerical_df = pd.DataFrame(numerical_s, columns=numerical_df.columns)\n",
    "    \n",
    "#     # 类别型特征 One-Hot 编码\n",
    "#     encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "#     categorical_s = encoder.fit_transform(categorical_df)\n",
    "#     categorical_df = pd.DataFrame(categorical_s, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "    \n",
    "#     # 合并处理后的数值型和类别型特征\n",
    "#     X_trains[i] = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "#     # 处理测试数据\n",
    "#     numerical_df_test = X_tests[i].select_dtypes(include=['float'])   # 数值型特征\n",
    "#     categorical_df_test = X_tests[i].select_dtypes(include=['int'])   # 类别型特征（可能为类别特征）\n",
    "    \n",
    "#     # 使用相同的 scaler 对测试数据进行标准化\n",
    "#     numerical_s_test = scaler.transform(numerical_df_test)\n",
    "#     numerical_df_test = pd.DataFrame(numerical_s_test, columns=numerical_df_test.columns)\n",
    "    \n",
    "#     # 使用相同的 encoder 对测试数据进行 One-Hot 编码\n",
    "#     categorical_s_test = encoder.transform(categorical_df_test)\n",
    "#     categorical_df_test = pd.DataFrame(categorical_s_test, columns=encoder.get_feature_names_out(categorical_df_test.columns))\n",
    "    \n",
    "#     # 合并处理后的数值型和类别型特征\n",
    "#     X_tests[i] = pd.concat([numerical_df_test, categorical_df_test], axis=1)\n",
    "\n",
    "#     # 选择前 20 个重要特征\n",
    "#     selector = SelectKBest(chi2, k=40)\n",
    "#     X_trains[i] = selector.fit_transform(X_trains[i], y_trains[i])\n",
    "#     X_tests[i] = selector.transform(X_tests[i])\n",
    "#     print(\"after encoding: \", X_trains[i].shape[1])\n",
    "# # 确保 y_trains 的标签转换为数值格式\n",
    "# for i in range(len(dataset_names)):\n",
    "#     y_trains[i].iloc[:, 0] = pd.to_numeric(y_trains[i].iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset_names)):\n",
    "#     missing_cols = set(X_trains[i].columns) - set(X_tests[i].columns)\n",
    "#     print(missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class YourTorchModel(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(YourTorchModel, self).__init__()\n",
    "        # 定義模型層\n",
    "        self.layer0 = nn.Linear(input_size, 128)\n",
    "        self.layer1 = nn.Linear(128, 32)\n",
    "        # self.layer2 = nn.Linear(16, 12)\n",
    "        # self.layer3 = nn.Linear(12, 8)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(12)\n",
    "        self.bn3 = nn.BatchNorm1d(8)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.layer2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.act_fn(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.act_fn(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return torch.sigmoid(x)  # Apply sigmoid activation here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# # 根據不同數據續集訓練模型\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用 stratify 將數據分為訓練集和測試集\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "#     input_size = tmp_X_train.shape[1]\n",
    "#     # 將數據轉換為 PyTorch 張量\n",
    "#     # 將 DataFrame 轉為 numpy 陣列再轉為 PyTorch 張量\n",
    "#     # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#     tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#     tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#     tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#     tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "#     # 初始化模型、損失函數和優化器\n",
    "#     # model = YourTorchModel(input_size)      #if you don't have cuda gpu\n",
    "#     model = YourTorchModel(input_size).to(device)\n",
    "#     criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "    \n",
    "#     l2_lambda = 0.5  # L2 regularization factor\n",
    "#     l1_lambda = 0.01  # L1 regularization factor   \n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  \n",
    "\n",
    "#     # 訓練迴圈\n",
    "#     num_epochs = 300\n",
    "    \n",
    "#     train_acc = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "\n",
    "#         # Apply L1 regularization\n",
    "#         l1_penalty = sum(param.abs().sum() for param in model.parameters())\n",
    "#         loss = loss + l1_lambda * l1_penalty\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #train acc\n",
    "#         preds = (outputs > 0.5).float()  # 將概率轉換為類別標籤\n",
    "#         train_acc += (preds == tmp_y_train).float().mean()  # 計算準確率\n",
    "\n",
    "#     # 評估 - 預測測試集的概率\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy() # 概率預測值\n",
    "    \n",
    "#     # 計算 AUC\n",
    "#     # 經過資料處理後，可能會出現只有一個類別的情況，此時 AUC 會報錯，因此這裡做了一個判斷\n",
    "#     unique_classes = np.unique(tmp_y_test.cpu().numpy())\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "#         auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "\n",
    "#     # print(f\"{dataset_names[i]} AUC: {auc}\")\n",
    "#     avg_auc += auc\n",
    "#     models.append(model)\n",
    "\n",
    "#     train_acc /= num_epochs\n",
    "#     # 在每個 dataset 結束時打印損失和準確率,and test auc\n",
    "#     print(f\"Dataset {dataset_names[i]}: Train Acc: {train_acc.item():.8f}, Test AUC: {auc:.8f}\")   \n",
    "\n",
    "# print(\"平均 AUC:\", avg_auc / len(dataset_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把所有資料訓練集合後訓練(效果不是很好)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# avg_auc = 0\n",
    "# num_epochs = 100  # 訓練迴圈放到最外層\n",
    "# # 把所有數據集放在一起訓練\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_train_acc = 0\n",
    "#     epoch_avg_auc = 0\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#     for i in range(len(dataset_names)):\n",
    "#         # 使用 stratify 將數據分為訓練集和測試集\n",
    "#         tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#             X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#         )\n",
    "#         input_size = tmp_X_train.shape[1]\n",
    "\n",
    "#         # 將數據轉換為 PyTorch 張量並移動到 CUDA (GPU)\n",
    "#         tmp_X_train = torch.tensor(tmp_X_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_train = torch.tensor(tmp_y_train.values, dtype=torch.float32).to(device)\n",
    "#         tmp_X_test = torch.tensor(tmp_X_test.values, dtype=torch.float32).to(device)\n",
    "#         tmp_y_test = torch.tensor(tmp_y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "#         # 初始化模型、損失函數和優化器\n",
    "#         # if epoch == 0 and i == 0:\n",
    "#         model = YourTorchModel(input_size).to(device)\n",
    "#         criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "#         l2_lambda = 0.1\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)\n",
    "\n",
    "#         # 訓練\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(tmp_X_train)\n",
    "#         loss = criterion(outputs, tmp_y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # 計算訓練準確率\n",
    "#         preds = (outputs > 0.5).float()\n",
    "#         train_acc = (preds == tmp_y_train).float().mean()\n",
    "#         epoch_train_acc += train_acc.item() / len(dataset_names)\n",
    "\n",
    "#         # 評估 - 預測測試集的概率\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tmp_y_prob = model(tmp_X_test).squeeze().cpu().numpy()\n",
    "\n",
    "#         # 計算 AUC\n",
    "#         auc = roc_auc_score(tmp_y_test.cpu().numpy(), tmp_y_prob)\n",
    "#         epoch_avg_auc += auc / len(dataset_names)\n",
    "\n",
    "#         # 保存模型（若在第一個 epoch 時，保存到模型列表中）\n",
    "#         if epoch == 0:\n",
    "#             models.append(model)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}: Avg Train Acc: {epoch_train_acc:.8f}, Avg Test AUC: {epoch_avg_auc:.8f}\")\n",
    "\n",
    "# print(\"整體平均 AUC:\", epoch_avg_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross vlidation(實驗中跑不動)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Define your parameter grid for XGBoost or any model you're tuning\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 250, 300],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "    \n",
    "    \n",
    "# }\n",
    "\n",
    "# avg_auc = 0\n",
    "# models = []\n",
    "\n",
    "# for i in range(len(dataset_names)):\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "#     )\n",
    "    \n",
    "#     # Cross-validation with parameter tuning\n",
    "#     model = XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='auc',\n",
    "#     tree_method='gpu_hist',\n",
    "#     use_label_encoder=False,\n",
    "#     predictor='gpu_predictor'\n",
    "#     )\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "#     grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "#     # Get the best model from cross-validation\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     models.append(best_model)\n",
    "\n",
    "#     # Evaluate the best model\n",
    "#     tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "#     unique_classes = np.unique(tmp_y_test)\n",
    "#     if len(unique_classes) < 2:\n",
    "#         print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "#         auc = 0.7\n",
    "#     else:\n",
    "#         auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "#     avg_auc += auc\n",
    "\n",
    "#     print(f\"AUC for dataset {i+1} with tuned parameters: {auc}\")\n",
    "\n",
    "# print(f\"Average AUC across all datasets: {avg_auc / len(dataset_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc of dataset  0: \t0.8632478632478633\n",
      "auc of dataset  1: \t1.0\n",
      "auc of dataset  2: \t0.7755102040816326\n",
      "auc of dataset  3: \t0.7577319587628866\n",
      "auc of dataset  4: \t0.9713114754098361\n",
      "auc of dataset  5: \t0.9957894736842104\n",
      "auc of dataset  6: \t0.975\n",
      "auc of dataset  7: \t0.8402777777777777\n",
      "auc of dataset  8: \t0.8380952380952381\n",
      "auc of dataset  9: \t0.7424242424242424\n",
      "auc of dataset 10: \t0.7\n",
      "auc of dataset 11: \t0.994949494949495\n",
      "auc of dataset 12: \t0.8111888111888113\n",
      "auc of dataset 13: \t1.0\n",
      "auc of dataset 14: \t0.615\n",
      "auc of dataset 15: \t1.0\n",
      "auc of dataset 16: \t1.0\n",
      "auc of dataset 17: \t0.9921875\n",
      "auc of dataset 18: \t0.9850088183421516\n",
      "auc of dataset 19: \t0.9791666666666667\n",
      "auc of dataset 20: \t0.9857142857142858\n",
      "auc of dataset 21: \t0.9375\n",
      "auc of dataset 22: \t0.9865319865319865\n",
      "auc of dataset 23: \t0.6383193277310923\n",
      "auc of dataset 24: \t1.0\n",
      "auc of dataset 25: \t0.8963210702341137\n",
      "auc of dataset 26: \t0.9931350114416476\n",
      "auc of dataset 27: \t0.8557291666666667\n",
      "auc of dataset 28: \t0.8571428571428572\n",
      "auc of dataset 29: \t0.7126760563380282\n",
      "auc of dataset 30: \t0.65\n",
      "auc of dataset 31: \t0.8174740484429066\n",
      "auc of dataset 32: \t1.0\n",
      "auc of dataset 33: \t0.9089673913043478\n",
      "auc of dataset 34: \t0.853125\n",
      "auc of dataset 35: \t1.0\n",
      "auc of dataset 36: \t0.9226006191950464\n",
      "auc of dataset 37: \t0.666\n",
      "auc of dataset 38: \t0.9994251221615407\n",
      "auc of dataset 39: \t0.853125\n",
      "auc of dataset 40: \t1.0\n",
      "auc of dataset 41: \t1.0\n",
      "auc of dataset 42: \t0.8174740484429066\n",
      "auc of dataset 43: \t0.9814814814814815\n",
      "auc of dataset 44: \t1.0\n",
      "auc of dataset 45: \t0.8993506493506493\n",
      "auc of dataset 46: \t0.736111111111111\n",
      "auc of dataset 47: \t1.0\n",
      "auc of dataset 48: \t0.9891232696110746\n",
      "avg auc :   0.8937595311741338\n"
     ]
    }
   ],
   "source": [
    "# 這邊用迴圈跑所有的資料集，並且將每個資料集的資料分成訓練集和測試集\n",
    "# 並且用 Random Forest 來做分類\n",
    "# 得到每個資料集的 AUC\n",
    "# for max_deep in max_deeps:\n",
    "max_deep  = 5\n",
    "models=[]\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "for i in range(len(dataset_names)):\n",
    "    # 這邊做一下 stratify\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, stratify=y_trains[i]\n",
    "    )\n",
    "\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=700, n_jobs=-1, random_state=2, max_depth=max_deep,  \n",
    "    # )\n",
    "    # XGBoost (好像比較容易過擬合，適合大資料集) \n",
    "   \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=400, \n",
    "        eta=0.01,\n",
    "        \n",
    "        gamma=1, \n",
    "        max_depth=10, \n",
    "        subsample=0.7, \n",
    "        colsample_bytree=0.6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        reg_lambda=1,     # L2 regularization term on weights\n",
    "        alpha=0.1         # L1 regularization term on weights\n",
    "    )\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    unique_classes = np.unique(tmp_y_test)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Skipping AUC calculation for this dataset due to single class in y_test.\")\n",
    "        auc = 0.7  # 或選擇其他合適的值，例如預設值\n",
    "    else:\n",
    "        tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    print(f'auc of dataset {i:2}: \\t{auc}')\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #給neural network 用\n",
    "# y_predicts = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # Get the test data and convert to tensor if needed\n",
    "#     X_test = torch.tensor(X_tests[i], dtype=torch.float32).to(device)  # Move to the same device as the model\n",
    "\n",
    "#     # Put model in evaluation mode\n",
    "#     models[i].eval()\n",
    "    \n",
    "#     # Disable gradient calculation for inference\n",
    "#     with torch.no_grad():\n",
    "#         # Pass data through the model and apply sigmoid to get probabilities\n",
    "#         logits = models[i](X_test)  # logits will be on the same device as the model\n",
    "#         y_predict_proba = torch.sigmoid(logits).cpu().numpy().flatten()  # Convert to numpy array\n",
    "\n",
    "#     # Store the predictions as a DataFrame\n",
    "#     df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "#     y_predicts.append(df)\n",
    "#     #print auc\n",
    "#     # print(f\"Dataset {dataset_names[i]}: Test AUC: {roc_auc_score(y_trains[i], y_predict_proba)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "##給xgboost用\n",
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    # print(X_tests[i])\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# test_aucs = []\n",
    "# for i in range(len(dataset_names)):\n",
    "#     # 使用模型進行預測，獲得類別 1 的預測機率\n",
    "#     y_predict_proba = models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    \n",
    "#     # 計算 AUC 分數\n",
    "#     auc = roc_auc_score(y_tests[i], y_predict_proba)\n",
    "#     print(f'AUC of dataset {i:2}: \\t{auc}')\n",
    "    \n",
    "#     test_aucs.append(auc)\n",
    "\n",
    "# # 平均 AUC\n",
    "# avg_test_auc = sum(test_aucs) / len(test_aucs)\n",
    "# print(\"\\nAverage Test AUC:\", avg_test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
