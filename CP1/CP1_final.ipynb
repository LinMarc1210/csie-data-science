{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "# 收集所有資料夾名稱，按照數字順序排序資料夾名稱\n",
    "for folder_name in os.listdir(\"../Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "dataset_names = sorted(dataset_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for folder_name in dataset_names:\n",
    "    # print(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"../Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"../Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"../Competition_data/{folder_name}/X_test.csv\",header=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(len(dataset_names))\n",
    "# print(len(X_trains))  # 49, 代表有 49 個 dataFrame (每個資料集各一個)\n",
    "# print(len(y_trains))\n",
    "# print(len(X_tests))\n",
    "# print(X_trains[0].dtypes)\n",
    "# print(y_trains[0].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering\n",
    "我們試了許多方法，如xgboost 選擇importance，PCA降維度， PYOD 的 IForest 檢測異常值，one hot encoding...等結果比什麼都不做來的差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL XGBoost為主要訓練model\n",
    "然後把準確率低的值重新用CV 調參數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將大家的參數存入JSON中，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加載現有的參數文件\n",
    "try:\n",
    "    with open('best_params.json', 'r') as file:\n",
    "        best_params = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # 如果文件不存在，初始化一個空字典\n",
    "    best_params = {}\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    best_params.setdefault(f\"dataset_{i+1}\", {\"auc\": 0, \"params\": {}})\n",
    "    # 保存更新後的 JSON 文件\n",
    "with open('best_params.json', 'w') as file:\n",
    "    json.dump(best_params, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for dataset 1 with tuned parameters: 0.8514246947082769\n",
      "AUC for dataset 2 with tuned parameters: 0.9995215311004785\n",
      "AUC for dataset 3 with tuned parameters: 0.7916666666666667\n",
      "\u001b[31mAUC for dataset 4 with tuned parameters: 0.44693877551020406\u001b[0m\n",
      "AUC for dataset 5 with tuned parameters: 0.9066666666666667\n",
      "AUC for dataset 6 with tuned parameters: 0.9551020408163265\n",
      "AUC for dataset 7 with tuned parameters: 0.9786780383795309\n",
      "AUC for dataset 8 with tuned parameters: 0.9418181818181819\n",
      "AUC for dataset 9 with tuned parameters: 0.8942307692307693\n",
      "AUC for dataset 10 with tuned parameters: 0.7934731934731934\n",
      "\u001b[31mAUC for dataset 11 with tuned parameters: 0.2857142857142857\u001b[0m\n",
      "AUC for dataset 12 with tuned parameters: 0.9904761904761904\n",
      "AUC for dataset 13 with tuned parameters: 0.96\n",
      "AUC for dataset 14 with tuned parameters: 1.0\n",
      "AUC for dataset 15 with tuned parameters: 0.7583333333333332\n",
      "AUC for dataset 16 with tuned parameters: 0.9995215311004785\n",
      "AUC for dataset 17 with tuned parameters: 0.9259259259259259\n",
      "AUC for dataset 18 with tuned parameters: 1.0\n",
      "AUC for dataset 19 with tuned parameters: 0.9920634920634921\n",
      "AUC for dataset 20 with tuned parameters: 0.9575757575757575\n",
      "AUC for dataset 21 with tuned parameters: 0.993006993006993\n",
      "AUC for dataset 22 with tuned parameters: 0.869281045751634\n",
      "AUC for dataset 23 with tuned parameters: 0.947695035460993\n",
      "\u001b[31mAUC for dataset 24 with tuned parameters: 0.6251709986320109\u001b[0m\n",
      "AUC for dataset 25 with tuned parameters: 1.0\n",
      "AUC for dataset 26 with tuned parameters: 0.8461538461538461\n",
      "AUC for dataset 27 with tuned parameters: 1.0\n",
      "AUC for dataset 28 with tuned parameters: 0.8564516129032258\n",
      "AUC for dataset 29 with tuned parameters: 0.947027027027027\n",
      "AUC for dataset 30 with tuned parameters: 0.7951153324287653\n",
      "AUC for dataset 31 with tuned parameters: 0.8636363636363635\n",
      "AUC for dataset 32 with tuned parameters: 0.8109579100145138\n",
      "AUC for dataset 33 with tuned parameters: 1.0\n",
      "AUC for dataset 34 with tuned parameters: 0.8571428571428571\n",
      "AUC for dataset 35 with tuned parameters: 0.8521505376344086\n",
      "AUC for dataset 36 with tuned parameters: 1.0\n",
      "AUC for dataset 37 with tuned parameters: 0.888888888888889\n",
      "AUC for dataset 38 with tuned parameters: 0.726\n",
      "AUC for dataset 39 with tuned parameters: 0.9930555555555556\n",
      "AUC for dataset 40 with tuned parameters: 0.8521505376344086\n",
      "AUC for dataset 41 with tuned parameters: 0.9935125115848008\n",
      "AUC for dataset 42 with tuned parameters: 1.0\n",
      "AUC for dataset 43 with tuned parameters: 0.8109579100145138\n",
      "AUC for dataset 44 with tuned parameters: 1.0\n",
      "AUC for dataset 45 with tuned parameters: 1.0\n",
      "AUC for dataset 46 with tuned parameters: 0.9523809523809523\n",
      "\u001b[31mAUC for dataset 47 with tuned parameters: 0.6071428571428571\u001b[0m\n",
      "AUC for dataset 48 with tuned parameters: 1.0\n",
      "AUC for dataset 49 with tuned parameters: 0.9884640738299276\n",
      "avg auc :   0.8878668147221286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models=[]\n",
    "retrain_index = []\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "smote = SMOTE(random_state=42)\n",
    "bad_threshold = 0.7\n",
    "params = {\n",
    "    \"n_estimators\": 700,\n",
    "    \"eta\": 0.008,\n",
    "    \"gamma\": 0.95,\n",
    "    \"scale_pos_weight\": 2,\n",
    "    \"max_depth\": 15,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"reg_lambda\": 1,            # L2 regularization term on weights\n",
    "    \"alpha\": 0.1                # L1 regularization term on weights\n",
    "}\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "\n",
    "    model = XGBClassifier( **params)\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    \n",
    "    tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "    auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    if(auc < bad_threshold):            # 如果 AUC 太低，就將這個資料集的 index 存起來，等等重新訓練\n",
    "        retrain_index.append(i)\n",
    "        print(f\"\\033[31mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\") \n",
    "    else:\n",
    "        print(f\"AUC for dataset {i+1} with tuned parameters: {auc}\")\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "    #將參數存入 JSON 文件\n",
    "    if auc >= best_params[f\"dataset_{i+1}\"][\"auc\"]:\n",
    "        best_params[f\"dataset_{i+1}\"][\"auc\"] = auc\n",
    "        best_params[f\"dataset_{i+1}\"][\"params\"].update(params)\n",
    "    with open('best_params.json', 'w') as file:\n",
    "        json.dump(best_params, file, indent=4)\n",
    "\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把準確率低的抓出來重新訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考量要跑很久，我們有直接將訓練好的參數記錄在best_params.json\n",
    "\n",
    "如果要自己逐步調整(一次調一兩個參數，要把下面xgboost 同樣名字的參數註解掉才不會衝突)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# param_grid = {\n",
    "    \n",
    "#     # 'learning_rate': [0.005, 0.01,0.05,0.1],\n",
    "#     # 'n_estimators': [300,400,500],         #500 400 is mostly good \n",
    "#     # 'max_depth': [ 2,3,5,6, 7,9,10,12],\n",
    "#     # 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#     # 'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#     # 'alpha': [0.01,0.1, 0.5,1.5,],\n",
    "#     # 'reg_lambda': [0.01,0.1, 0.5,2],\n",
    "#     # 'gamma': [0.1,0.8, 0.95, 1, 1.5,],\n",
    "#     # 'scale_pos_weight' : [1,2,3,5],\n",
    "# }\n",
    "# print(retrain_index)\n",
    "\n",
    "# for i in retrain_index:\n",
    "#     tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "#         X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "#     )\n",
    "#     tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "#     model = XGBClassifier(\n",
    "#     #     n_estimators=700, \n",
    "#     #     eta=0.008,\n",
    "        \n",
    "#     #     # gamma=0.95,    # 1 is best\n",
    "#     #     min_child_weight = 2,\n",
    "#     #     # scale_pos_weight=2,\n",
    "#     #     max_depth=15, \n",
    "#     #     subsample=0.5, \n",
    "#     #     colsample_bytree=0.9,\n",
    "#     #     objective='binary:logistic',\n",
    "#     #     eval_metric='logloss',\n",
    "#     #     # reg_lambda=1,     # L2 regularization term on weights\n",
    "#     #     # alpha=0.1         # L1 regularization term on weights\n",
    "#     **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {})  # Load the best parameters from the JSON file\n",
    "#     )\n",
    "        \n",
    "    \n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "#     grid_search.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "   \n",
    "#     # Get the best model from cross-validation\n",
    "#     best_model = grid_search.best_estimator_\n",
    "    \n",
    "#     print(f\"Best parameters for dataset {i+1}: {grid_search.best_params_}\")\n",
    "\n",
    "#     # Evaluate the best model\n",
    "#     tmp_y_prob = best_model.predict_proba(tmp_X_test)[:, 1]\n",
    "   \n",
    "#     auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "#     #Save the best parameters to JSON\n",
    "#     # 更新或追加參數(if auc better)\n",
    "#     # if auc >= best_params.get(f\"dataset_{i+1}\", {}).get('auc', 0):\n",
    "#     #     print(f\"\\033[32mAUC for dataset {i+1} with tuned parameters: {auc}\\033[0m\")\n",
    "#     #     best_params[f\"dataset_{i+1}\"]['auc'] = auc\n",
    "\n",
    "#     #     if f\"dataset_{i+1}\" in best_params:\n",
    "#     #         best_params[f\"dataset_{i+1}\"][\"params\"].update(grid_search.best_params_)\n",
    "#     #     else:\n",
    "#     #         best_params[f\"dataset_{i+1}\"][\"params\"]  = grid_search.best_params_\n",
    "#     # else:\n",
    "#     print(f\"AUC for retrain dataset {i+1} with tuned parameters: {auc}\")\n",
    "#     # Save the best parameters to a JSON file\n",
    "    \n",
    "# with open('best_params.json', 'w') as file:\n",
    "#     json.dump(best_params, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc of dataset 1: \t0.8514246947082769\n",
      "auc of dataset 2: \t0.9995215311004785\n",
      "auc of dataset 3: \t0.7916666666666667\n",
      "auc of dataset 4: \t0.44693877551020406\n",
      "auc of dataset 5: \t0.9066666666666667\n",
      "auc of dataset 6: \t0.9551020408163265\n",
      "auc of dataset 7: \t0.9786780383795309\n",
      "auc of dataset 8: \t0.9418181818181819\n",
      "auc of dataset 9: \t0.8942307692307693\n",
      "auc of dataset 10: \t0.7934731934731934\n",
      "auc of dataset 11: \t0.37142857142857144\n",
      "auc of dataset 12: \t0.9904761904761904\n",
      "auc of dataset 13: \t0.96\n",
      "auc of dataset 14: \t1.0\n",
      "auc of dataset 15: \t0.7583333333333332\n",
      "auc of dataset 16: \t0.9995215311004785\n",
      "auc of dataset 17: \t0.9259259259259259\n",
      "auc of dataset 18: \t1.0\n",
      "auc of dataset 19: \t0.9920634920634921\n",
      "auc of dataset 20: \t0.9575757575757575\n",
      "auc of dataset 21: \t0.993006993006993\n",
      "auc of dataset 22: \t0.869281045751634\n",
      "auc of dataset 23: \t0.947695035460993\n",
      "auc of dataset 24: \t0.6251709986320109\n",
      "auc of dataset 25: \t1.0\n",
      "auc of dataset 26: \t0.8461538461538461\n",
      "auc of dataset 27: \t1.0\n",
      "auc of dataset 28: \t0.8564516129032258\n",
      "auc of dataset 29: \t0.947027027027027\n",
      "auc of dataset 30: \t0.7951153324287653\n",
      "auc of dataset 31: \t0.8636363636363635\n",
      "auc of dataset 32: \t0.8109579100145138\n",
      "auc of dataset 33: \t1.0\n",
      "auc of dataset 34: \t0.8571428571428571\n",
      "auc of dataset 35: \t0.8521505376344086\n",
      "auc of dataset 36: \t1.0\n",
      "auc of dataset 37: \t0.888888888888889\n",
      "auc of dataset 38: \t0.726\n",
      "auc of dataset 39: \t0.9930555555555556\n",
      "auc of dataset 40: \t0.8521505376344086\n",
      "auc of dataset 41: \t0.9935125115848008\n",
      "auc of dataset 42: \t1.0\n",
      "auc of dataset 43: \t0.8109579100145138\n",
      "auc of dataset 44: \t1.0\n",
      "auc of dataset 45: \t1.0\n",
      "auc of dataset 46: \t0.9523809523809523\n",
      "auc of dataset 47: \t0.6071428571428571\n"
     ]
    }
   ],
   "source": [
    "##最後重新train model by best params\n",
    "\n",
    "# 讀取現有 JSON 文件\n",
    "with open('best_params.json', 'r') as file:\n",
    "    best_params = json.load(file)\n",
    "\n",
    "models=[]\n",
    "avg_auc = 0\n",
    "avg_train = 0\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "  \n",
    "    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42, \n",
    "    )\n",
    "    tmp_X_train,tmp_y_train = smote.fit_resample(tmp_X_train, tmp_y_train)\n",
    "    model = XGBClassifier(\n",
    "        **best_params.get(f\"dataset_{i+1}\", {}).get(\"params\", {}),  # Load the best parameters from the JSON file\n",
    "    )\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "   \n",
    "    tmp_y_prob = model.predict_proba(tmp_X_test)[:, 1]\n",
    "    auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "    print(f'auc of dataset {i+1}: \\t{auc}')\n",
    "    avg_auc += auc\n",
    "    models.append(model)\n",
    "        \n",
    "print(f\"avg auc :   {avg_auc / len(dataset_names)}\")\n",
    "# print(f\"avg auc of maxdeep of {max_deep}:   {avg_auc / len(dataset_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##給xgboost用\n",
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    # print(X_tests[i])\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'../Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
